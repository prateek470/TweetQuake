{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Project \n",
    "TweetQuake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "# nltk.download()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 1\n",
    "data.Info[('Not related' == data.Informativeness)] = 0\n",
    "# data.head()\n",
    "X = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return nostop\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['no-stopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word))\n",
    "    return stemmed_word\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.Tweet_Text.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "    \n",
    "# Feature Extraction\n",
    "# X['count'] = X.Tweet_Text.str.split(' ').apply(len)#.value_counts()\n",
    "# X['has_earthquake'] = X.Tweet_Text.str.contains('earthquake').apply(int)\n",
    "def Feature_extraction_A(X):\n",
    "    X['temp_1'] = X.Tweet_Text.str.split(' ').apply(len)\n",
    "    X['temp_2'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "    X['feature_a'] = X.temp_1.astype(str) + ' words, the ' + X.temp_2.astype(str) + ' word'\n",
    "    X = X.drop('temp_1', axis=1)\n",
    "    X = X.drop('temp_2', axis=1)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_B(X):\n",
    "    X['feature_b'] = X.Tweet_Text.str.split().apply(remove_punc)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_C(X):\n",
    "    X['feature_c'] = X.Tweet_Text.str.split().apply(find_before_after_query_word)\n",
    "    return X\n",
    "\n",
    "def remove_punc(value):\n",
    "    punctuation_marks = re.compile(r'[.?!,\":;#-]')\n",
    "    words = []\n",
    "    for each in value:\n",
    "        words.append(punctuation_marks.sub('',each))\n",
    "    return ','.join(words)\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_before_after_query_word(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            if i == 0 and len(val)>1:\n",
    "                return ', '+val[i+1]\n",
    "            elif i == len(val)-1 and len(val)>1:\n",
    "                return val[i-1]+', '\n",
    "            else:\n",
    "                return val[i-1]+', '+val[i+1]\n",
    "    return ', '\n",
    "    \n",
    "# X['position'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "# X['before_query1'] = 0\n",
    "# X.before_query1[X.position > 0] = X.position - 1\n",
    "X = remove_stopword(X)\n",
    "X = stemming (X)\n",
    "X = Feature_extraction_A(X)\n",
    "X = Feature_extraction_B(X)\n",
    "X = Feature_extraction_C(X)\n",
    "X\n",
    "\n",
    "\n",
    "\n",
    "import twitter, json\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "# results = api.GetSearch(\n",
    "#     raw_query=\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "# for each in results:\n",
    "# #     print(json.dumps(each, indent=2))\n",
    "#     print each\n",
    "\n",
    "# Find tweets using tweet ID\n",
    "res = api.GetStatus(389949367009808384)\n",
    "\n",
    "# Code to get the new data with Location and Created date\n",
    "# def find_created_at(id):\n",
    "#     try:\n",
    "#         res = api.GetStatus(id)\n",
    "#         return res.created_at\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['created_at'] = X.Tweet_ID.astype(int).apply(find_created_at)\n",
    "# def find_location(id):\n",
    "#     try:\n",
    "#         return api.GetStatus(id).user.location\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['location'] = X.Tweet_ID.astype(int).apply(find_location)\n",
    "# X.to_csv('new_data.csv', sep=',',encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration of Twitter API\n",
    "Keys are for reference \n",
    "We can get new data set using this\n",
    "or modify the old data set to get some new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, json\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "# results = api.GetSearch(\n",
    "#     raw_query=\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "# for each in results:\n",
    "# #     print(json.dumps(each, indent=2))\n",
    "#     print each\n",
    "\n",
    "# Find tweets using tweet ID\n",
    "res = api.GetStatus(389949367009808384)\n",
    "\n",
    "# Code to get the new data with Location and Created date\n",
    "# def find_created_at(id):\n",
    "#     try:\n",
    "#         res = api.GetStatus(id)\n",
    "#         return res.created_at\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['created_at'] = X.Tweet_ID.astype(int).apply(find_created_at)\n",
    "# def find_location(id):\n",
    "#     try:\n",
    "#         return api.GetStatus(id).user.location\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['location'] = X.Tweet_ID.astype(int).apply(find_location)\n",
    "# X.to_csv('new_data.csv', sep=',',encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}