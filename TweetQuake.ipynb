{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Final Project\n",
    "## TweetQuake\n",
    "Detection of Earthquake using Twitter data.\n",
    "Twitter users are everywhere, check the twitter map and earthquake frequency map\n",
    "<img src=\"img/TwitterMap.png\" alt=\"Drawing\" style=\"width: 475px;float:left; margin-top: 30px\" title = \"Twitter Map\"/>\n",
    "<img src=\"img/EarthQuakeMap.png\" alt=\"Drawing\" style=\"width: 475px;float:right\" title = \"Earthquake Map\"/>\n",
    "<img src=\"img/EarthQuakeTweet.png\" alt=\"Drawing\" style=\"width: 600px;\" title = \"Earthquake Tweet Frequency\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "%matplotlib inline\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip().lower())\n",
    "\n",
    "# data.head()\n",
    "X = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordmap = defaultdict(int)\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        #word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return ' '.join(nostop)\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['nostopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word)) \n",
    "    return ' '.join(stemmed_word)\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.nostopword.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "\n",
    "def update_wordmap(tweet):\n",
    "    #update wordmap\n",
    "    for word in tweet:\n",
    "        wordmap[word]+=1\n",
    "\n",
    "def term_frequency_plot(X):\n",
    "    #Plot a graph for term frequency in tweets\n",
    "    X.stem.str.split().apply(update_wordmap)\n",
    "    sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    objects = list()\n",
    "    freq = list()\n",
    "    for i in range(10):\n",
    "        objects.append(sorted_x[i][0])\n",
    "        freq.append(sorted_x[i][1])\n",
    "\n",
    "    x_pos = np.arange(len(objects))\n",
    "    plt.barh(x_pos, freq, align='center', alpha=0.5)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.yticks(x_pos, objects)\n",
    "    plt.title('Term Frequency usage')\n",
    "    plt.show()\n",
    "    \n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.Tweet_Text.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_B(X):\n",
    "    word_features = X[['Tweet_ID','Tweet_Text','Info']]\n",
    "    word_features = word_features.values.tolist()\n",
    "\n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "\n",
    "    for tweet in word_features:\n",
    "        if tweet[2] == 'related':\n",
    "            data_pos.append(tweet[1])\n",
    "        else:\n",
    "            data_neg.append(tweet[1])\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 token_pattern=token_pattern,\n",
    "                                 binary=True,\n",
    "                                max_features=5000)\n",
    "    word_vector = vectorizer.fit_transform(data_pos+data_neg)\n",
    "    \n",
    "    return X.join(pd.DataFrame(word_vector.toarray()))\n",
    "\n",
    "def Feature_extraction_C(X):\n",
    "#     X['feature_c'] = X.Tweet_Text.str.split().apply(find_before_after_query_word)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def remove_punc(value):\n",
    "    punctuation_marks = re.compile(r'[.?!,\":;#-]')\n",
    "    words = []\n",
    "    for each in value:\n",
    "        words.append(punctuation_marks.sub('',each))\n",
    "    return ','.join(words)\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find(porter_stemmer.stem('earthquake')) != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_before_after_query_word(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            if i == 0 and len(val)>1:\n",
    "                return ','+val[i+1]\n",
    "            elif i == len(val)-1 and len(val)>1:\n",
    "                return val[i-1]+','\n",
    "            else:\n",
    "                return val[i-1]+','+val[i+1]\n",
    "    return ', '\n",
    "\n",
    "X = remove_stopword(X)\n",
    "X = stemming (X)\n",
    "term_frequency_plot(X)\n",
    "X = Feature_extraction_A(X)\n",
    "# X = Feature_extraction_B(X)\n",
    "# X = Feature_extraction_C(X)\n",
    "# X\n",
    "\n",
    "y = data['Info'].values\n",
    "X = X.drop('Tweet_ID',axis=1)\n",
    "X = X.drop('Tweet_Text',axis=1)\n",
    "\n",
    "X = X.drop('nostopword',axis=1)\n",
    "X = X.drop('stem',axis=1)\n",
    "\n",
    "\n",
    "X = X.drop('Info',axis=1)\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration of Twitter API\n",
    "Keys are for reference \n",
    "We can get new data set using this\n",
    "or modify the old data set to get some new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = '859531338973622277-fdJp7rien3doiULaof2DwcLwIzngo6k'\n",
    "access_token_secret = 'h3hAd5kzn9qRngThgQyRm9t2p1ErZH1orpAQ4HA15dlG9'\n",
    "consumer_key = 'wBJ2csLxjMCz0hRwWF7Pw826z'\n",
    "consumer_secret ='Au0OUtezSVY5VjKrBo9XTz9HJRHIQw2dJPtAVA4K1qBZgGGfh2'\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(StdOutListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print self.num_tweets\n",
    "        if self.num_tweets < 5:\n",
    "            print json.loads(data)['id']\n",
    "            self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    print tweepy.API(auth).get_user('twitter').screen_name\n",
    "    print tweepy.API(auth).me().name\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(track=['night'])\n",
    "#     stream.filter()\n",
    "    \n",
    "get_tweets()\n",
    "# print tweepy.API(auth).get_user('twitter').screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SVM model implementation\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# vec = DictVectorizer()\n",
    "# df = vec.fit_transform(X.feature_a.str).toarray()\n",
    "# X = pd.get_dummies(X.feature_b.str.split())\n",
    "\n",
    "def standardizedX(X):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    standardizedX = scaler.transform(X)\n",
    "    return standardizedX\n",
    "# Tune hyperparameter gamma and choose best gamma for model training\n",
    "def hyperparameter_tuning(X, y):\n",
    "\t# Choose value of hyper parameter from below values of gamma\n",
    "    gammas = [2**-1, 2**-3, 2**-5, 2**-7, 2**-9]\n",
    "    classifier = GridSearchCV(estimator=svm.SVR(), cv=10, param_grid=dict(gamma=gammas))\n",
    "\n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 10- fold cross validation and error evaluation\n",
    "# Loop of 30 to see 300 validations with shuffle on and off\n",
    "# Loop of 10 to check the confidence interval - paper doesn't describe much on the \n",
    "# confidence interval. This works only for shuffle On.\n",
    "def cross_validation_evaluation(X, y):\n",
    "    mean_error, mad_error = 0, 0\n",
    "    count = 0\n",
    "    mean_min, mad_min = 100, 100\n",
    "    mean_max, mad_max = 0, 0\n",
    "    f1_sco,accuracy_sco = 0,0\n",
    "#     classifier = hyperparameter_tuning(X, y)\n",
    "    model=svm.SVC(kernel='linear', gamma=0.001)#classifier.best_estimator_.gamma)\n",
    "    \n",
    "    for j in range(1):\n",
    "        for i in range(1):\n",
    "            kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "#             print len(X)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                count += 1\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "#                 mean_error += mean_squared_error(y_test, model.predict(X_test))\n",
    "#                 mad_error += mean_absolute_error(y_test,model.predict(X_test))\n",
    "#                 print (y_test,model.predict(X_test))\n",
    "#                 print (model.predict(X_test))\n",
    "                f1_sco += f1_score(y_test,model.predict(X_test),pos_label='related')\n",
    "                accuracy_sco += accuracy_score(y_test,model.predict(X_test),normalize=True)\n",
    "#                 print classification_report(y_test,model.predict(X_test))\n",
    "#         mean_min = min(mean_min, (mean_error/count)**0.5)\n",
    "#         mean_max = max(mean_max, (mean_error/count)**0.5)\n",
    "#         mad_min = min(mad_min, (mad_error/count))\n",
    "#         mad_max = max(mad_max, (mad_error/count))\n",
    "#     RMSE = (mean_error/count)**0.5\n",
    "#     MAD = mad_error/count\n",
    "    print 'F1-score: ' + str(f1_sco/count)\n",
    "    print 'Accuracy: ' + str(accuracy_sco/count)\n",
    "#     return RMSE, MAD, mean_min, mean_max, mad_min, mad_max\n",
    "    \n",
    "\n",
    "# X = standardizedX(X)\n",
    "# print X.tail()\n",
    "cross_validation_evaluation(X.values, y)\n",
    "# RMSE, MAD, mean_min, mean_max, mad_min, mad_max = cross_validation_evaluation(X.values, y)\n",
    "# print \"For feature: \"\n",
    "# print 'RMSE +- Confidence Interval: '+\"{0:.2f}\".format(RMSE)+' +- '+\"{0:.2f}\".format(max(RMSE-mean_min, mean_max-RMSE))\n",
    "# print 'MAD +- Confidence Interval: '+\"{0:.2f}\".format(MAD)+' +- '+\"{0:.2f}\".format(max(MAD-mad_min, mad_max-MAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive-Bayes SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        p = np.asarray(p,dtype=np.float)\n",
    "        q = np.asarray(q,dtype=np.float)\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + \\\n",
    "                self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + \\\n",
    "                     self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_imdb():\n",
    "    print(\"Vectorizing Training Text\")\n",
    "    data = pd.read_csv(\"2013_Bohol_earthquake-tweets_labeled.csv\")\n",
    "    data['Info'] = 'related'\n",
    "    data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "    data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                              encode('ascii', 'ignore').\\\n",
    "                                              strip())\n",
    "    X = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "    X = X.values.tolist()\n",
    "\n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "\n",
    "    for tweet in X:\n",
    "        if tweet[2] == 'related':\n",
    "            data_pos.append(tweet[1])\n",
    "        else:\n",
    "            data_neg.append(tweet[1])\n",
    "\n",
    "    train_pos = data_pos[:1100]\n",
    "    train_neg = data_neg[:1100]\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 token_pattern=token_pattern,\n",
    "                                 binary=True)\n",
    "    X_train = vectorizer.fit_transform(train_pos+train_neg)\n",
    "    y_train = np.array([1]*len(train_pos)+[0]*len(train_neg))\n",
    "\n",
    "    print(\"Vocabulary Size: %s\" % len(vectorizer.vocabulary_))\n",
    "    print(\"Vectorizing Testing Text\")\n",
    "\n",
    "    test_pos = data_pos[1100:]\n",
    "    test_neg = data_neg[1100:]\n",
    "\n",
    "    X_test = vectorizer.transform(test_pos + test_neg)\n",
    "    y_test = np.array([1]*len(test_pos)+[0]*len(test_neg))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def main():\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_imdb()\n",
    "\n",
    "    print(\"Fitting Model\")\n",
    "\n",
    "    mnbsvm = NBSVM()\n",
    "    mnbsvm.fit(X_train, y_train)\n",
    "    print('Test Accuracy: %s' % mnbsvm.score(X_test, y_test))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "pd.set_option('display.max_rows', 2500)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}