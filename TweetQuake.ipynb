{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Final Project\n",
    "## TweetQuake\n",
    "Detection of Earthquake using Twitter data.\n",
    "Twitter users are everywhere, check the twitter map and earthquake frequency map\n",
    "<img src=\"img/TwitterMap.png\" alt=\"Drawing\" style=\"width: 475px;float:left; margin-top: 30px\" title = \"Twitter Map\"/>\n",
    "<img src=\"img/EarthQuakeMap.png\" alt=\"Drawing\" style=\"width: 475px;float:right\" title = \"Earthquake Map\"/>\n",
    "<img src=\"img/EarthQuakeTweet.png\" alt=\"Drawing\" style=\"width: 600px;\" title = \"Earthquake Tweet Frequency\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "%matplotlib inline\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip())\n",
    "\n",
    "# data.head()\n",
    "X = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordmap = defaultdict(int)\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = str()\n",
    "    for word in tweet:\n",
    "        #word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop+= \" \" + word\n",
    "    return nostop\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['nostopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = str()\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word += \" \" + (porter_stemmer.stem(word))\n",
    "    return stemmed_word\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.nostopword.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "\n",
    "def update_wordmap(tweet):\n",
    "    #update wordmap\n",
    "    for word in tweet:\n",
    "        wordmap[word]+=1\n",
    "\n",
    "def term_frequency_plot(X):\n",
    "    #Plot a graph for term frequency in tweets\n",
    "    X.stem.str.split().apply(update_wordmap)\n",
    "    sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    objects = list()\n",
    "    freq = list()\n",
    "    for i in range(10):\n",
    "        objects.append(sorted_x[i][0])\n",
    "        freq.append(sorted_x[i][1])\n",
    "\n",
    "    x_pos = np.arange(len(objects))\n",
    "    plt.barh(x_pos, freq, align='center', alpha=0.5)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.yticks(x_pos, objects)\n",
    "    plt.title('Term Frequency usage')\n",
    "    plt.show()\n",
    "    \n",
    "# Feature Extraction\n",
    "# X['count'] = X.Tweet_Text.str.split(' ').apply(len)#.value_counts()\n",
    "def has_earthquake(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1 or val[i].lower().find('quake') != -1:\n",
    "            return 1\n",
    "    return 0\n",
    "X['has_earthquake'] = X.Tweet_Text.str.split().apply(has_earthquake)\n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.Tweet_Text.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "#     X['feature_a'] = X.temp_1.astype(str) + ' words, the ' + X.temp_2.astype(str) + ' word'\n",
    "#     X = X.drop('temp_1', axis=1)\n",
    "#     X = X.drop('temp_2', axis=1)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_B(X):\n",
    "    X['feature_b'] = X.Tweet_Text.str.split().apply(remove_punc)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_C(X):\n",
    "    X['feature_c'] = X.Tweet_Text.str.split().apply(find_before_after_query_word)\n",
    "    return X\n",
    "\n",
    "def remove_punc(value):\n",
    "    punctuation_marks = re.compile(r'[.?!,\":;#-]')\n",
    "    words = []\n",
    "    for each in value:\n",
    "        words.append(punctuation_marks.sub('',each))\n",
    "    return ','.join(words)\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_before_after_query_word(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            if i == 0 and len(val)>1:\n",
    "                return ','+val[i+1]\n",
    "            elif i == len(val)-1 and len(val)>1:\n",
    "                return val[i-1]+','\n",
    "            else:\n",
    "                return val[i-1]+','+val[i+1]\n",
    "    return ', '\n",
    "    \n",
    "# X['position'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "# X['before_query1'] = 0\n",
    "# X.before_query1[X.position > 0] = X.position - 1\n",
    "X = remove_stopword(X)\n",
    "X = stemming (X)\n",
    "term_frequency_plot(X)\n",
    "X = Feature_extraction_A(X)\n",
    "# X = Feature_extraction_B(X)\n",
    "# X = Feature_extraction_C(X)\n",
    "# X\n",
    "\n",
    "y = data['Info'].values\n",
    "X = X.drop('Tweet_ID',axis=1)\n",
    "X = X.drop('Tweet_Text',axis=1)\n",
    "\n",
    "X = X.drop('nostopword',axis=1)\n",
    "X = X.drop('stem',axis=1)\n",
    "\n",
    "\n",
    "X = X.drop('Info',axis=1)\n",
    "X\n",
    "# X = X.drop('total_words',axis=1)\n",
    "# X = X.drop('position_query_word',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration of Twitter API\n",
    "Keys are for reference \n",
    "We can get new data set using this\n",
    "or modify the old data set to get some new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, json\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "results = api.GetSearch(\n",
    "    raw_query=\"q=-earthquake%2C%20-shaking%2C%20-quake%2C%20-tremor%20since%3A2010-09-15%20until%3A2017-04-23&src=typd&lang=en\")#\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "\n",
    "# f = csv.writer(open(\"test_neg1.csv\", \"wb+\"))\n",
    "# f.writerow([\"tweet_id\", \"tweet_text\", \"created_at\", \"url\", \"location\",\"media\"])\n",
    "current_id = ''\n",
    "# print results\n",
    "for x in results:\n",
    "#     print x.text\n",
    "    x.text = x.text.encode('utf-8').strip()\n",
    "    x.user.location = x.user.location.encode('utf-8').strip()\n",
    "#     if x.media is not None:\n",
    "#     x.media = x.media.encode('utf-8').strip()\n",
    "#     if x.urls is not None and x.media is not None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,1,x.user.location,1])\n",
    "#     elif x.urls is None and x.media is not None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,0,x.user.location,1])\n",
    "#     elif x.urls is not None and x.media is None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,1,x.user.location,0])\n",
    "#     else:\n",
    "#         f.writerow([x.id,x.text,x.created_at,0,x.user.location,0])\n",
    "#     f.writerow([x.id,x.text,x.created_at,x.urls,x.user.location,x.media])\n",
    "    current_id = x.id\n",
    "count = 0\n",
    "for i in range(0):\n",
    "    results = api.GetSearch(\n",
    "        raw_query=\"q=-earthquake%2C%20-shaking%2C%20-quake%2C%20-tremor%20since%3A2010-11-18%20until%3A2017-04-23&src=typd&count=100&lang=en&since_id\"+str(current_id))#\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "    for x in results:\n",
    "        x.text = x.text.encode('utf-8').strip()\n",
    "        x.user.location = x.user.location.encode('utf-8').strip()\n",
    "#         x.media = x.media.encode('utf-8').strip()\n",
    "#         if x.urls is not None and x.media is not None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,1,x.user.location,1])\n",
    "#         elif x.urls is None and x.media is not None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,0,x.user.location,1])\n",
    "#         elif x.urls is not None and x.media is None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,1,x.user.location,0])\n",
    "#         else:\n",
    "#             f.writerow([x.id,x.text,x.created_at,0,x.user.location,0])\n",
    "        f.writerow([x.id,x.text,x.created_at,x.urls,x.user.location,x.media])\n",
    "        current_id = x.id\n",
    "        count += 1\n",
    "#     print count\n",
    "\n",
    "# Find tweets using tweet ID\n",
    "res = api.GetStatus(389949367009808384)\n",
    "\n",
    "# Code to get the new data with Location and Created date\n",
    "# def find_created_at(id):\n",
    "#     try:\n",
    "#         res = api.GetStatus(id)\n",
    "#         return res.created_at\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['created_at'] = X.Tweet_ID.astype(int).apply(find_created_at)\n",
    "# def find_location(id):\n",
    "#     try:\n",
    "#         return api.GetStatus(id).user.location\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['location'] = X.Tweet_ID.astype(int).apply(find_location)\n",
    "# X.to_csv('new_data.csv', sep=',',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SVM model implementation\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# vec = DictVectorizer()\n",
    "# df = vec.fit_transform(X.feature_a.str).toarray()\n",
    "# X = pd.get_dummies(X.feature_b.str.split())\n",
    "\n",
    "def standardizedX(X):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    standardizedX = scaler.transform(X)\n",
    "    return standardizedX\n",
    "# Tune hyperparameter gamma and choose best gamma for model training\n",
    "def hyperparameter_tuning(X, y):\n",
    "\t# Choose value of hyper parameter from below values of gamma\n",
    "    gammas = [2**-1, 2**-3, 2**-5, 2**-7, 2**-9]\n",
    "    classifier = GridSearchCV(estimator=svm.SVR(), cv=10, param_grid=dict(gamma=gammas))\n",
    "\n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 10- fold cross validation and error evaluation\n",
    "# Loop of 30 to see 300 validations with shuffle on and off\n",
    "# Loop of 10 to check the confidence interval - paper doesn't describe much on the \n",
    "# confidence interval. This works only for shuffle On.\n",
    "\n",
    "def cross_validation_evaluation(X, y):\n",
    "    mean_error, mad_error = 0, 0\n",
    "    count = 0\n",
    "    mean_min, mad_min = 100, 100\n",
    "    mean_max, mad_max = 0, 0\n",
    "    f1_sco,accuracy_sco = 0,0\n",
    "#     classifier = hyperparameter_tuning(X, y)\n",
    "    #classifier.best_estimator_.gamma)\n",
    "    model=svm.SVC(kernel='linear')\n",
    "    for j in range(1):\n",
    "        for i in range(10):\n",
    "            kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "#             print len(X)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                count += 1\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "#                 mean_error += mean_squared_error(y_test, model.predict(X_test))\n",
    "#                 mad_error += mean_absolute_error(y_test,model.predict(X_test))\n",
    "#                 print (y_test,model.predict(X_test))\n",
    "#                 print (model.predict(X_test))\n",
    "                f1_sco += f1_score(y_test,model.predict(X_test),pos_label='related')\n",
    "                accuracy_sco += accuracy_score(y_test,model.predict(X_test),normalize=True)\n",
    "#                 print classification_report(y_test,model.predict(X_test))\n",
    "#         mean_min = min(mean_min, (mean_error/count)**0.5)\n",
    "#         mean_max = max(mean_max, (mean_error/count)**0.5)\n",
    "#         mad_min = min(mad_min, (mad_error/count))\n",
    "#         mad_max = max(mad_max, (mad_error/count))\n",
    "#     RMSE = (mean_error/count)**0.5\n",
    "#     MAD = mad_error/count\n",
    "    print 'F1-score: ' + str(f1_sco/count)\n",
    "    print 'Accuracy: ' + str(accuracy_sco/count)\n",
    "#     return RMSE, MAD, mean_min, mean_max, mad_min, mad_max\n",
    "    return model\n",
    "    \n",
    "\n",
    "# X = standardizedX(X)\n",
    "# print X.tail()\n",
    "model = cross_validation_evaluation(X.values, y)\n",
    "\n",
    "# RMSE, MAD, mean_min, mean_max, mad_min, mad_max = cross_validation_evaluation(X.values, y)\n",
    "# print \"For feature: \"\n",
    "# print 'RMSE +- Confidence Interval: '+\"{0:.2f}\".format(RMSE)+' +- '+\"{0:.2f}\".format(max(RMSE-mean_min, mean_max-RMSE))\n",
    "# print 'MAD +- Confidence Interval: '+\"{0:.2f}\".format(MAD)+' +- '+\"{0:.2f}\".format(max(MAD-mad_min, mad_max-MAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for testing the Earthquake\n",
    "1. Given a set of queries Q for a target event.\n",
    "2. Put a query Q using search API every s seconds and obtain tweets T.\n",
    "3. For each tweet t 2 T, obtain features A, B, and C. Apply the classification to obtain value vt ¼ f0; 1g.\n",
    "4. If the enough number of tweets comes(poccur in (1) exceeds 0.99 under the condition: 10 tweets in 10 minutes; \u0002 ¼ 0:34; pf ¼ 0:35;) then proceed to step 5.\n",
    "5. For each tweet t 2 T, we obtain the latitude and the longitude lt by 1) using the associated GPS location, 2) making a query to Google Map for the registered location for user ut. Set lt ¼ null if neither functions.\n",
    "6. Calculate the estimated location of the event from lt; t 2 T using normal particle filtering, particle filtering with assigned weights, and particle filtering with weights and sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, json, time\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "try:\n",
    "    current_id = 856275770427269120\n",
    "    count = 0\n",
    "    while count < 3:\n",
    "        results = api.GetSearch(raw_query=\"q=earthquake%2C%20OR%20shaking&lang=en&since_id=\"+str(current_id))\n",
    "        count += 1\n",
    "        # Loop through each of the earthquakes that were returned\n",
    "        df = pd.DataFrame(columns=['Tweet_ID','Tweet_Text'])\n",
    "        df.index=range(len(df))\n",
    "        for each in results:\n",
    "            each.text = each.text.encode('utf-8').strip()\n",
    "            data = pd.DataFrame({'Tweet_ID':[each.id], 'Tweet_Text':[each.text]})\n",
    "#             print data\n",
    "            print each.text\n",
    "            df = df.append(data)\n",
    "            \n",
    "            # Sleep for a minute so we don't spam Twitter in case of many earthquakes at once\n",
    "            current_id = max(current_id, each.id)\n",
    "        \n",
    "        X = df[['Tweet_Text']]\n",
    "        X['has_earthquake'] = X.Tweet_Text.str.split().apply(has_earthquake)\n",
    "        X = remove_stopword(X)\n",
    "        X = stemming (X)\n",
    "        X = Feature_extraction_A(X)\n",
    "\n",
    "        X = X.drop('Tweet_Text',axis=1)\n",
    "        X = X.drop('nostopword',axis=1)\n",
    "        X = X.drop('stem',axis=1)\n",
    "#         df[['Tweet_Text']]\n",
    "        X\n",
    "        print model.predict(X.values)\n",
    "        # Sleep for an hour so we don't spam Twitter\n",
    "        time.sleep(30)\n",
    "except Exception as e:\n",
    "    print(\"Error ocurred\")\n",
    "    print(e)\n",
    "print current_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "856275770427269120"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
