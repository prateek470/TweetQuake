{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Final Project\n",
    "## TweetQuake\n",
    "Detection of Earthquake using Twitter data.\n",
    "Twitter users are everywhere, check the twitter map and earthquake frequency map\n",
    "<img src=\"img/TwitterMap.png\" alt=\"Drawing\" style=\"width: 475px;float:left; margin-top: 30px\" title = \"Twitter Map\"/>\n",
    "<img src=\"img/EarthQuakeMap.png\" alt=\"Drawing\" style=\"width: 475px;float:right\" title = \"Earthquake Map\"/>\n",
    "<img src=\"img/EarthQuakeTweet.png\" alt=\"Drawing\" style=\"width: 600px;\" title = \"Earthquake Tweet Frequency\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Problem Statement\n",
    "\n",
    "Online Social Media have become an important alternate channel to spread information during natural disasters and other important events. Twitter is one such channel where people look forward to knowing about their friends and relatives after a catastrophic event such as an Earthquake, tornado, and others. The real-time nature of Twitter enables it to be an important source for information extraction. In a case of an earthquake, people who experience it, tweet about it to inform their friends about their safety and to spread the news of a target event. In this report, we have investigated the extraction of event information about Earthquake using the service API of Twitter. Our algorithm classifies Tweets on the basis of whether they are relevant to Earthquake and if yes, whether they are indicative of a current earthquake.\n",
    "\n",
    "\n",
    "Earlier, the main source of information for an earthquake like event was the radio, however, there was some time lapse from the occurrence of an event and the announcement and also the reach of information was not global. In the past few years, people especially youngsters are turning to Twitter to learn about such events. Twitter has the advantage of fast propagation speed and more number of contributors when compared to radio. \n",
    "\n",
    "Twitter is categorized as a microblogging service, ie a form of blogging that enables users to send brief text and multimedia updates. Other similar services are Tumblr, Plurk etc. We focus on examining the data received from Twitter due to its popularity and data volume. There are 190 million people who use Twitter per month, generating 65 million tweets per day[1] . Compared to other microblogging sites, Twitter user updates their account more often and tweet regularly, often several times in a day. For example, when an airplane crash-landed on the Hudson River in New York, the first published report was via Twitter. Similarly, many events such as presidential campaigns, football games, and others are being analyzed more often via social media tools such as Twitter rather than radio or News channel. \n",
    "\n",
    "Generally, people who experience an event such as Earthquake are the first to report it via Twitter, and the tweets spread out even before the event is registered with the USGS and long before it is covered in news channels. The motivation behind our research was:\n",
    "\tCan we detect such event occurrence in real-time by monitoring tweets?\n",
    "The amount of real-time information flow via Twitter motivated us to design a real-time automated Earthquake Detection Classifier. \n",
    "\n",
    "## Related Work.\n",
    "\n",
    "Event or disaster detection using twitter is one of the fastest and effective way to get the relevant information. Many researchers used the tweets to get the information about the trends, social media relations of twitter users, retweet activities. Earthquake detection using tweets is first observed in 2010 (Tweet Analysis for Real-Time Event Detection and Earthquake Reporting System Development\n",
    ")[8]\n",
    "Since than USGS developed an application to effectively detect an earthquake using as minimum as 14 tweets. Currently, twitter is being used to detect an earthquake along with other disasters. \\\n",
    "\n",
    "Previously many researchers have published studies of twitter and observed the network structure of Twitter [2], [3], [4]. Some of the researchers used social media as of the characteristics to analyze the Twitter data [5], [6]. In papers [7] and [8], authors have development different applications using the tweet and other relevant data of Twitter and developed some real time applications.\n",
    "\n",
    "\n",
    "Most of the papers on event detection use basic features like presence of a query word like earthquake or shaking along with the length of tweet (as the person experiencing tweet will not write a big and fancy tweet). The accuracy of the previous event detection as described in the paper [8] is just 76% in best case. We have improved the feature selection process and used different classifiers to detect the earthquake relevant tweets and current earthquake related tweets to improve the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach / Methods.\n",
    "\n",
    "We have used SVM classifier to classify the tweets. \n",
    "\n",
    "Data Collection\n",
    "We have collected previous earthquake related data from different below sources\n",
    "http://crisislex.org/data-collections.html\n",
    "Twitter dev API\n",
    "https://earthquake.usgs.gov/earthquakes/browse/significant.php \n",
    "Manual labeling of tweets from Twitter.\n",
    "\n",
    "### 3 Unique Contributions (Compared to the research paper)\n",
    "1. Two-step classification - To mimic the structure of classes in data.\n",
    "2. Using NBSVM classifer  - A combination of Naive Bayes and SVM.\n",
    "3. Creative features for data related to our domain.\n",
    "\n",
    "Since it was difficult to get proper training data of good quality for this problem, we manuall collected the Twitter data by querying the earthquake time tweets which contains some information about earthquake and manually labelled than as earthquake indicative or not. \n",
    "\n",
    "Initial Exploration\n",
    "Initially the we used the data collected from crisislex and observed very high accuracy as the earthquake relevant data has earthquake keyword and the irrelevant tweet does not have the keyword. We observed that the Model was overfitting the data. After taking the effort to manually collect and label data, our classifier improved.\n",
    "\n",
    "Revised/Different approaches\n",
    "The approach described in the paper was to find three basic features like the presence of query word, the position of query word in the tweet and the words in the tweet along with the word before and after the query word. The data used in the paper is all positive samples and the final accuracy was 73 percent which is same by using only the feature A (presence of query word in the tweet) and using all three features. \n",
    "\n",
    "We used two different classifiers to find the earthquake 'related' and earthquake 'relevant' tweet and the accuracy observed is 87 percent by using the best training data set. Our model runs in real time by getting the tweets using twitter API and classifying them as earthquake related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGHCAYAAAD1HvUOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlKlIIZ7b/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1160a7f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip().lower())\n",
    "\n",
    "X1 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y1 = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordmap = defaultdict(int)\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        #word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return ' '.join(nostop)\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['nostopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word)) \n",
    "    return ' '.join(stemmed_word)\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.nostopword.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "\n",
    "def update_wordmap(tweet):\n",
    "    #update wordmap\n",
    "    for word in tweet:\n",
    "        wordmap[word]+=1\n",
    "\n",
    "def term_frequency_plot(X):\n",
    "    #Plot a graph for term frequency in tweets\n",
    "    X.stem.str.split().apply(update_wordmap)\n",
    "    sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    objects = list()\n",
    "    freq = list()\n",
    "    for i in range(10):\n",
    "        objects.append(sorted_x[i][0])\n",
    "        freq.append(sorted_x[i][1])\n",
    "\n",
    "    x_pos = np.arange(len(objects))\n",
    "    plt.barh(x_pos, freq, align='center', alpha=0.5)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.yticks(x_pos, objects)\n",
    "    plt.title('Term Frequency usage')\n",
    "    plt.show()\n",
    "    \n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.stem.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.stem.str.split().apply(find_position)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_BnC(X):\n",
    "    word_features = X[['Tweet_ID','Tweet_Text','Info']]\n",
    "    word_features = word_features.values.tolist()\n",
    "\n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "\n",
    "    for tweet in word_features:\n",
    "        if tweet[2] == 'related':\n",
    "            data_pos.append(tweet[1])\n",
    "        else:\n",
    "            data_neg.append(tweet[1])\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 token_pattern=token_pattern,\n",
    "                                 binary=True,\n",
    "                                max_features=5000)\n",
    "    word_vector = vectorizer.fit_transform(data_pos+data_neg)\n",
    "    \n",
    "    return X.join(pd.DataFrame(word_vector.toarray()))\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find(porter_stemmer.stem('earthquake')) != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "X1 = remove_stopword(X1)\n",
    "X1 = stemming (X1)\n",
    "term_frequency_plot(X1)\n",
    "X1 = Feature_extraction_A(X1)\n",
    "# X1 = Feature_extraction_BnC(X1)\n",
    "\n",
    "y1 = data['Info'].values\n",
    "X1 = X1.drop('Tweet_ID',axis=1)\n",
    "X1 = X1.drop('Tweet_Text',axis=1)\n",
    "X1 = X1.drop('nostopword',axis=1)\n",
    "X1 = X1.drop('stem',axis=1)\n",
    "X1 = X1.drop('Info',axis=1)\n",
    "X1 = X1.values\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X1)\n",
    "type(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(X1)\n",
    "X1 = scaler1.transform(X1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=SVC(C='rbf', cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([  2.50000e-01,   2.00000e+00,   1.60000e+01,   1.28000e+02,\n",
       "         1.02400e+03]), 'gamma': array([  1.95312e-03,   1.10485e-02,   6.25000e-02,   3.53553e-01,\n",
       "         2.00000e+00])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 128.0, 'gamma': 2.0}\n"
     ]
    }
   ],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "# k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range)\n",
    "grid1 = GridSearchCV(SVC('rbf'), param_grid=params_grids, cv=10)\n",
    "grid1.fit(X1_train,y1_train)\n",
    "\n",
    "print grid1.best_params_\n",
    "# grid1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=128.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=2.0, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.79962894248608529"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.99397590361445787"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.60661764705882348"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.75342465753424659"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = SVC(kernel='rbf' ,C=grid1.best_params_['C'], \n",
    "             gamma=grid1.best_params_['gamma'])\n",
    "\n",
    "clf1.fit(X1_train,y1_train)\n",
    "y1_pred = clf1.predict(X1_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y1_test, y1_pred)\n",
    "sklearn.metrics.precision_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.recall_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.f1_score(y1_test, y1_pred,pos_label='related')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive-Bayes SVM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes SVM (NBSVM) is a simple but novel SVM variant using NB log-count ratios as feature values and is supposed to be a robust performer. This model is an interpolation between MNB and SVM, which can be seen as a form of regularization: trust NB unless the SVM is very confident. \n",
    "\n",
    "The concept of NBSVM has been obtained from this research paper : Wang, Sida, and Christopher D. Manning. \"Baselines and bigrams: Simple, good sentiment and topic classification.\" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 2012.\n",
    "\n",
    "The class implementation of the NBSVM has been obtained from this repository: https://github.com/Joshua-Chin/nbsvm.git\n",
    "\n",
    "For the first classification, where we are classifying earthquake-relevant tweets from earthquake-irrelevant tweets, we are using the NBSVM classifier as it's giving us a higher accuracy compared to the SVM classifier discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        p = np.asarray(p,dtype=np.float)\n",
    "        q = np.asarray(q,dtype=np.float)\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + \\\n",
    "                self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + \\\n",
    "                     self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are diving the data into a train-test split of 80-20. We are building our vocabulary based on the training data using scikit-learn's method TfidfVectorizer. We are using unigram, bigram and trigram features and the Tf-Idf weighting scheme on the word vector. We are setting the tf term in tf-idf to be binary, as it was increasing our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Training Text\n",
      "Vocabulary Size: 29936\n",
      "Vectorizing Testing Text\n",
      "Fitting Model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NBSVM(C=1, alpha=1, beta=0.25, fit_intercept=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.969450101833\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Vectorizing Training Text\")\n",
    "\n",
    "X2 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "X2 = X2.values.tolist()\n",
    "\n",
    "data_pos = []\n",
    "data_neg = []\n",
    "\n",
    "for tweet in X2:\n",
    "    if tweet[2] == 'related':\n",
    "        data_pos.append(tweet[1])\n",
    "    else:\n",
    "        data_neg.append(tweet[1])\n",
    "\n",
    "train_pos = data_pos[:1100]\n",
    "train_neg = data_neg[:1100]\n",
    "\n",
    "token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                             token_pattern=token_pattern,\n",
    "                             binary=True)\n",
    "\n",
    "X2_train = vectorizer.fit_transform(train_pos+train_neg)\n",
    "y2_train = np.array([1]*len(train_pos)+[0]*len(train_neg))\n",
    "\n",
    "print(\"Vocabulary Size: %s\" % len(vectorizer.vocabulary_))\n",
    "print(\"Vectorizing Testing Text\")\n",
    "\n",
    "test_pos = data_pos[1100:]\n",
    "test_neg = data_neg[1100:]\n",
    "\n",
    "X2_test = vectorizer.transform(test_pos+test_neg)\n",
    "y2_test = np.array([1]*len(test_pos)+[0]*len(test_neg))\n",
    "\n",
    "print(\"Fitting Model\")\n",
    "\n",
    "mnbsvm = NBSVM()\n",
    "mnbsvm.fit(X2_train, y2_train)\n",
    "print('Test Accuracy: %s' % mnbsvm.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Part 2: Classifying \"Earthquake Sensing\"  vs. \"Earthquake Relevant\" Tweets\n",
    "\n",
    "In the analysis up until now, we have been able to find and classify tweets which are *earthquake-relevant* vs *earthquake-irrelevant* tweets. However that is not enough to be able to detect earthquakes real-time. Many tweets that are *earthquake-relevant* are tweets in the aftermath of an earthquake when the occurence has been detected much before by seismic sensors and has been distributed by media houses and the internet. Many *earthquake-relevant* tweets might just be information about earthquakes or conferences and what not. Hence the next crucial step is to classify these 2 classes. \n",
    "\n",
    "*Positive Class - Earthquake Sensing Tweets*\n",
    "\n",
    "*Negative Class- (only)Earthquake relevant Tweets*\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "For this part of the analysis we did not find any data readily available for the *positive class*. For the *negative class* we could just use the data from the earlier dataset. So after trying and failing to extract data from twitter api and trying to sift through that and tag it, we *manually* scraped data off twitter and recorded it in a csv. **This task required significant effort**. We collected tweets by searching for known earthquake timings and locations and searched twitter for tweets which happened within 2-3 minutes of the earthquake. This would serve as the *positive class* for this training set. The total size of this data is ~ 800 tweets\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "If one eyeballs tweets from the *positive class* one sees that almost all of them contain the word earthquake. Also most of these tweets are small because when people react in the moment, they typically are too surprised to churn out long tweets. Thus *Feature 1* from previous analysis , i.e length of tweet and position of word \"earthquake\" are good features. This is what the paper we referred to had also used.\n",
    "\n",
    "In addition to those, **we came up with a few features of our own. This is another contribution above and beyond what was reported in the research paper**. \n",
    "\n",
    "1. If one peruses through the tweets, one sees that the tweets tweeted in the moments the earthquake is happening will almost never have any *url link*. There are 2 reasons. One is that again the user is too startled to cite stuff while experiencing an earthquake. Secondly since the first tweeters are by name the first people to know or experience the earthquake, there is no formal news or information available at that moment to refer to via links. In contrast many tweets which are relevant but not sensing are tweets done in the afterhours of an earthquake. Here many people link to either news articles, videos, other information on the internet. Hence *the presence/absence of url/links* is vital discriminatory information between the 2 classes, thus we make this a feature. We used regex to find http in our tweets.\n",
    "\n",
    "\n",
    "2. Similarly when users are tweeting about an earthquake as it is happening, nobody has any quantitative information regarding the magnitude/strength of the earthquake. Thus tweets would hardly have any numerical information except probably the time of occurence or other qualifiers like which time the earthquake is happening in a few months or years. BUT when tweets are tweeted in the afterhours of the earthquake, most of them contain the magnitude of the earthquake which is common information at that point of time. Hence again *the presence/absence of magnitude* is a pretty strong discriminator between positive and negative clases. Thus we chose this a a feature too. We use regex to find tweets with decimal values in tweets, which we believe (and saw from data) that it was a pretty good surrogate to finding magnitude specifically.\n",
    "\n",
    "**Classification and Evaluation**\n",
    "\n",
    "For classifiers we tried Support Vector Machines and Random Forests on our 4-feature dataset. Both these algorithms have many hyperparameters to tune and for that we used K-fold cross validation accuracy score. This was performed on only the *training data* (80% of total data) to find the best hyperparameter set. Then the complete training data was trained over this set of hyperparameters. Finally to evaluate performance, we tested the final classifer on our *test data* (20% of total data). Our performance from both classifiers was similar, and the SVM just performed marginally better. So we only included that in our analysis below.\n",
    "\n",
    "For evaluation we primarly check accuracy but also report precision, recall and F1-score (balanced f-score). For this classifier recall is more important that precision. This is because the recall controls how early (minimum number of tweets) we are able to raise an alarm given users start tweeting about an earthquake. Thus we want to be able to find maximum out of all such tweets. A bad precision will just lead to false alarms, the cost of which is significantly less than missing out on many tweets in an attempt to be precise. However our classifier performs equally good on all counts with more than 85% accuracy and almost similar (~85%) precision and recall. Thus it will pick 4 out of 5 tweets about an earthquake correctly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data = pd.read_csv('Earthquake_sensing_tweets.csv',header=0,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_url(tweet):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    if len(urls) !=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def has_magnitude(tweet):\n",
    "    decimal = re.findall(\"\\d+\\.\\d\\s\", tweet)\n",
    "    if len(decimal) != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data['has_magn']= tweet_sensing_data.Tweet_Text.apply(has_magnitude)\n",
    "tweet_sensing_data['has_url']= tweet_sensing_data.Tweet_Text.apply(find_url)\n",
    "\n",
    "tweet_sensing_data = remove_stopword(tweet_sensing_data)\n",
    "tweet_sensing_data = stemming (tweet_sensing_data)\n",
    "tweet_sensing_data = Feature_extraction_A(tweet_sensing_data)\n",
    "\n",
    "tweet_sensing_data = tweet_sensing_data.drop('nostopword',axis=1)\n",
    "tweet_sensing_data = tweet_sensing_data.drop('stem',axis=1)\n",
    "# tweet_sensing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataArray = tweet_sensing_data.values\n",
    "X3 = dataArray[:,3:]\n",
    "X3 = np.array(X3, dtype='float')\n",
    "y3 = dataArray[:,2]\n",
    "y3 = np.array(y3, dtype='float')\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(X3)\n",
    "X3 = scaler2.transform(X3)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ['linear', 'poly', 'rbf'], 'C': array([  2.50000e-01,   2.00000e+00,   1.60000e+01,   1.28000e+02,\n",
       "         1.02400e+03]), 'gamma': array([  1.95312e-03,   1.10485e-02,   6.25000e-02,   3.53553e-01,\n",
       "         2.00000e+00])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 2.0, 'gamma': 2.0}\n"
     ]
    }
   ],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range, kernel=k_options)\n",
    "grid2 = GridSearchCV(SVC(), param_grid=params_grids, cv=10)\n",
    "grid2.fit(X3_train,y3_train)\n",
    "\n",
    "print grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=2.0, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.84662576687116564"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.82894736842105265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.83999999999999997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.83443708609271527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classificaiton only with both feature set A from paper and our features.\n",
    "\n",
    "clf2 =  SVC(kernel=grid2.best_params_['kernel'] ,C=grid2.best_params_['C'], \n",
    "             gamma=grid2.best_params_['gamma'])\n",
    "clf2.fit(X3_train,y3_train)\n",
    "y3_pred = clf2.predict(X3_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y3_test, y3_pred)\n",
    "sklearn.metrics.precision_score(y3_test, y3_pred)\n",
    "sklearn.metrics.recall_score(y3_test, y3_pred)\n",
    "sklearn.metrics.f1_score(y3_test, y3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=2.0, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.66871165644171782"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.58139534883720934"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.73529411764705876"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classificaiton only with feature set A from paper\n",
    "clf2.fit(X3_train[:,:2],y3_train)\n",
    "y3_pred = clf2.predict(X3_test[:,:2])\n",
    "\n",
    "sklearn.metrics.accuracy_score(y3_test, y3_pred)\n",
    "sklearn.metrics.precision_score(y3_test, y3_pred)\n",
    "sklearn.metrics.recall_score(y3_test, y3_pred)\n",
    "sklearn.metrics.f1_score(y3_test, y3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=2.0, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.83435582822085885"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.83333333333333337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.81632653061224503"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classificaiton with only our features.\n",
    "\n",
    "clf2.fit(X3_train[:,2:],y3_train)\n",
    "y3_pred = clf2.predict(X3_test[:,2:])\n",
    "\n",
    "sklearn.metrics.accuracy_score(y3_test, y3_pred)\n",
    "sklearn.metrics.precision_score(y3_test, y3_pred)\n",
    "sklearn.metrics.recall_score(y3_test, y3_pred)\n",
    "sklearn.metrics.f1_score(y3_test, y3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can see that our features peform better and improve tbe classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Putting it together: Querying twitter real-time and finding \"earthquake sensing tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For querying twitter real time, we are using the python 'tweepy' module. We are extracting 50 live streaming tweets from twitter and saving them in a 'live_data.csv' file for later classificaiton. To this 'live_data.csv' file, we are initially appending a set of 10 tweets, out of which 5 of them indicate a current earthquake and the other 5 of them, though related to earthquake, do not indicate a current earthquake. Our classifier needs to be able to detect the tweets which indicate only a current earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = '859531338973622277-fdJp7rien3doiULaof2DwcLwIzngo6k'\n",
    "access_token_secret = 'h3hAd5kzn9qRngThgQyRm9t2p1ErZH1orpAQ4HA15dlG9'\n",
    "consumer_key = 'wBJ2csLxjMCz0hRwWF7Pw826z'\n",
    "consumer_secret ='Au0OUtezSVY5VjKrBo9XTz9HJRHIQw2dJPtAVA4K1qBZgGGfh2'\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(StdOutListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.f = csv.writer(open(\"live_data.csv\", \"wb+\"))\n",
    "        self.f.writerow([\"Tweet_Text\"])\n",
    "        self.f.writerow([\"Are we having an earthquake?\"])\n",
    "        self.f.writerow([\"EARTHQUAKE?\"])\n",
    "        self.f.writerow([\"It shook like crazy #earthquake\"])\n",
    "        self.f.writerow([\"WOAHHHHH that was my first earthquake!!!!!\"])\n",
    "        self.f.writerow([\"Is it just me or was that an earthquake?\"])\n",
    "        self.f.writerow([\"An earthquake of mag 8.2 shook Delhi yesterday!!\"])\n",
    "        self.f.writerow([\"RT biggest earthquake in last ten years!! Mag 9.1 richter reported\"])\n",
    "        self.f.writerow([\"Attending an earthquake conference today.\"])\n",
    "        self.f.writerow([\"Japan has frequent earthquakes.\"])\n",
    "        self.f.writerow([\"Which is worse? Earthquake of 7.2 or 8.1?\"])\n",
    "        \n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 50:\n",
    "#             print self.num_tweets\n",
    "            tweet_data = json.loads(data)\n",
    "            if 'text' in tweet_data and tweet_data['lang'] == 'en':\n",
    "                self.f.writerow([tweet_data['text'].encode('utf-8')])\n",
    "                self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            print('Done extracting')\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(locations=[-180,-90,180,90])\n",
    "    \n",
    "get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets that indicate a current earthquake : \n",
      "\n",
      "Are we having an earthquake?\n",
      "EARTHQUAKE?\n",
      "It shook like crazy #earthquake\n",
      "WOAHHHHH that was my first earthquake!!!!!\n",
      "Is it just me or was that an earthquake?\n",
      "Attending an earthquake conference today.\n"
     ]
    }
   ],
   "source": [
    "live_data = pd.read_csv('live_data.csv',header=0)\n",
    "\n",
    "print('Tweets that indicate a current earthquake : \\n')\n",
    "\n",
    "for tweet in live_data.values.tolist():\n",
    "    temp_tweet = tweet[0]\n",
    "    temp_tweet_vector = vectorizer.transform([temp_tweet])\n",
    "    if mnbsvm.predict(temp_tweet_vector)[0] == 1:\n",
    "#         print temp_tweet\n",
    "        tweet_df = pd.DataFrame([temp_tweet],columns=['Tweet_Text'])\n",
    "        tweet_df['has_magn']= tweet_df.Tweet_Text.apply(has_magnitude)\n",
    "        tweet_df['has_url']= tweet_df.Tweet_Text.apply(find_url)\n",
    "\n",
    "        tweet_df = remove_stopword(tweet_df)\n",
    "        tweet_df = stemming (tweet_df)\n",
    "        tweet_df = Feature_extraction_A(tweet_df)\n",
    "\n",
    "        tweet_df = tweet_df.drop('nostopword',axis=1)\n",
    "        tweet_df = tweet_df.drop('stem',axis=1)\n",
    "#         print tweet_df\n",
    "        tempArray = tweet_df.values\n",
    "        temp_X = tempArray[:,1:]\n",
    "        temp_X = np.array(temp_X, dtype='float')\n",
    "\n",
    "        temp_X = scaler2.transform(temp_X)\n",
    "        temp_y = clf2.predict(temp_X)\n",
    "        if temp_y == 1:\n",
    "            print temp_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "*Summary of tasks*\n",
    "\n",
    "1. Searched for data online and obtained 1 pre-curated dataset of tweets related to earthquakes.\n",
    "\n",
    "2. For doing exactly what the paper had done (and also a useful analysis rather than just classification), manually collected data from twitter(Earthquake_sensing_tweets.csv). This was painful but adds value to the project.\n",
    "\n",
    "3. Replicated features(A,B,C) and classifier(SVM) of the paper.\n",
    "\n",
    "4. Used NBSVM to obtained much better performance than paper.\n",
    "\n",
    "5. Due to data needs and conceptually different thoughts, we did a 2 step classification unlike the paper.\n",
    "\n",
    "6. Based on data exploration, created 2 new features which increased the performance of classification much more than that of Feature A described in paper.\n",
    "\n",
    "\n",
    "*Experience and Learnings*\n",
    "\n",
    "1. In analysis, spending time with data crucial and so is data exploration. Better features might get much better performace than better algorithms.\n",
    "\n",
    "2. Understand data needs much before starting analysis. Figure out automated data capture and if not, manual capture will take much more time than anticipated.\n",
    "\n",
    "3. When doing analysis on Jupyter notebook in a team, set variable and functional convention strongly. Merging takes forever otherwise and lot of code will need to be refactored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "[1] M. Sarah, C. Abdur, H. Gregor, L. Ben, and M. Roger, “Twitter and the Micro-Messaging Revolution,” technical report, O’Reilly Radar, 2008.\n",
    "\n",
    "[2] A. Java, X. Song, T. Finin, and B. Tseng, “Why We Twitter: Understanding Microblogging Usage and Communities,” Proc. Ninth WebKDD and First SNA-KDD Workshop Web Mining and Social Network Analysis (WebKDD/SNA-KDD ’07), pp. 56-65, 2007.\n",
    "\n",
    "[3] B. Huberman, D. Romero, and F. Wu, “Social Networks that Matter: Twitter Under the Microscope,” ArXiv E-Prints, http://arxiv.org/abs/0812.1045, Dec. 2008.\n",
    "\n",
    "[4] H. Kwak, C. Lee, H. Park, and S. Moon, “What is Twitter, A Social Network or A News Media?” Proc. 19th Int’l Conf. World Wide Web (WWW ’10), pp. 591-600, 2010.\n",
    "\n",
    "[5] G.L. Danah Boyd and S. Golder, “Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter,” Proc. 43rd Hawaii Int’l Conf. System Sciences (HICSS-43), 2010.\n",
    "\n",
    "[6] A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M. Welpe, “Predicting Elections with Twitter: What 140 Characters Reveal About Political Sentiment,” Proc. Fourth Int’l AAAI Conf. Weblogs and Social Media (ICWSM), 2010.\n",
    "\n",
    "[7] P. Galagan, “Twitter as a Learning Tool. Really,” ASTD Learning Circuits, p. 13, 2009.\n",
    "[8] K. Borau, C. Ullrich, J. Feng, and R. Shen, “Microblogging for Language Learning: Using Twitter to Train Communicative and Cultural Competence,” Proc. Eighth Int’l Conf. Advances in Web Based Learning (ICWL ’09), pp. 78-87, 2009.\n",
    "\n",
    "[8] Sakaki, Takeshi, Makoto Okazaki, and Yutaka Matsuo. \"Tweet analysis for real-time event detection and earthquake reporting system development.\" IEEE Transactions on Knowledge and Data Engineering 25.4 (2013): 919-931."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
