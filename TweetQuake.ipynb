{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Final Project\n",
    "## TweetQuake\n",
    "Detection of Earthquake using Twitter data.\n",
    "Twitter users are everywhere, check the twitter map and earthquake frequency map\n",
    "<img src=\"img/TwitterMap.png\" alt=\"Drawing\" style=\"width: 475px;float:left; margin-top: 30px\" title = \"Twitter Map\"/>\n",
    "<img src=\"img/EarthQuakeMap.png\" alt=\"Drawing\" style=\"width: 475px;float:right\" title = \"Earthquake Map\"/>\n",
    "<img src=\"img/EarthQuakeTweet.png\" alt=\"Drawing\" style=\"width: 600px;\" title = \"Earthquake Tweet Frequency\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip().lower())\n",
    "\n",
    "X1 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y1 = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordmap = defaultdict(int)\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        #word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return ' '.join(nostop)\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['nostopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word)) \n",
    "    return ' '.join(stemmed_word)\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.nostopword.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "\n",
    "def update_wordmap(tweet):\n",
    "    #update wordmap\n",
    "    for word in tweet:\n",
    "        wordmap[word]+=1\n",
    "\n",
    "def term_frequency_plot(X):\n",
    "    #Plot a graph for term frequency in tweets\n",
    "    X.stem.str.split().apply(update_wordmap)\n",
    "    sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    objects = list()\n",
    "    freq = list()\n",
    "    for i in range(10):\n",
    "        objects.append(sorted_x[i][0])\n",
    "        freq.append(sorted_x[i][1])\n",
    "\n",
    "    x_pos = np.arange(len(objects))\n",
    "    plt.barh(x_pos, freq, align='center', alpha=0.5)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.yticks(x_pos, objects)\n",
    "    plt.title('Term Frequency usage')\n",
    "    plt.show()\n",
    "    \n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.stem.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.stem.str.split().apply(find_position)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_BnC(X):\n",
    "    word_features = X[['Tweet_ID','Tweet_Text','Info']]\n",
    "    word_features = word_features.values.tolist()\n",
    "\n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "\n",
    "    for tweet in word_features:\n",
    "        if tweet[2] == 'related':\n",
    "            data_pos.append(tweet[1])\n",
    "        else:\n",
    "            data_neg.append(tweet[1])\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 token_pattern=token_pattern,\n",
    "                                 binary=True,\n",
    "                                max_features=5000)\n",
    "    word_vector = vectorizer.fit_transform(data_pos+data_neg)\n",
    "    \n",
    "    return X.join(pd.DataFrame(word_vector.toarray()))\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find(porter_stemmer.stem('earthquake')) != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "X1 = remove_stopword(X1)\n",
    "X1 = stemming (X1)\n",
    "term_frequency_plot(X1)\n",
    "X1 = Feature_extraction_A(X1)\n",
    "# X1 = Feature_extraction_BnC(X1)\n",
    "\n",
    "y1 = data['Info'].values\n",
    "X1 = X1.drop('Tweet_ID',axis=1)\n",
    "X1 = X1.drop('Tweet_Text',axis=1)\n",
    "X1 = X1.drop('nostopword',axis=1)\n",
    "X1 = X1.drop('stem',axis=1)\n",
    "X1 = X1.drop('Info',axis=1)\n",
    "X1 = X1.values\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X1)\n",
    "type(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(X1)\n",
    "X1 = scaler1.transform(X1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "# k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range)\n",
    "grid1 = GridSearchCV(SVC('rbf'), param_grid=params_grids, cv=10)\n",
    "grid1.fit(X1_train,y1_train)\n",
    "\n",
    "print grid1.best_params_\n",
    "# grid1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y1_pred = SVC(kernel=grid1.best_params_['kernel'] ,C=grid1.best_params_['C'], \n",
    "#              gamma=grid1.best_params_['gamma']).fit(X1_train,y1_train).predict(X1_test)\n",
    "\n",
    "clf1 = SVC(kernel='rbf' ,C=grid1.best_params_['C'], \n",
    "             gamma=grid1.best_params_['gamma'])\n",
    "\n",
    "clf1.fit(X1_train,y1_train)\n",
    "y1_pred = clf1.predict(X1_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y1_test, y1_pred)\n",
    "sklearn.metrics.precision_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.recall_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.f1_score(y1_test, y1_pred,pos_label='related')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive-Bayes SVM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes SVM (NBSVM) is a simple but novel SVM variant using NB log-count ratios as feature values and is supposed to be a robust performer. This model is an interpolation between MNB and SVM, which can be seen as a form of regularization: trust NB unless the SVM is very confident. \n",
    "\n",
    "The concept of NBSVM has been obtained from this research paper : Wang, Sida, and Christopher D. Manning. \"Baselines and bigrams: Simple, good sentiment and topic classification.\" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 2012.\n",
    "\n",
    "The class implementation of the NBSVM has been obtained from this repository: https://github.com/Joshua-Chin/nbsvm.git\n",
    "\n",
    "For the first classification, where we are classifying earthquake-relevant tweets from earthquake-irrelevant tweets, we are using the NBSVM classifier as it's giving us a higher accuracy compared to the SVM classifier discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        p = np.asarray(p,dtype=np.float)\n",
    "        q = np.asarray(q,dtype=np.float)\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + \\\n",
    "                self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + \\\n",
    "                     self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are diving the data into a train-test split of 80-20. We are building our vocabulary based on the training data using scikit-learn's method TfidfVectorizer. We are using unigram, bigram and trigram features and the Tf-Idf weighting scheme on the word vector. We are setting the tf term in tf-idf to be binary, as it was increasing our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Vectorizing Training Text\")\n",
    "\n",
    "X2 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "X2 = X2.values.tolist()\n",
    "\n",
    "data_pos = []\n",
    "data_neg = []\n",
    "\n",
    "for tweet in X2:\n",
    "    if tweet[2] == 'related':\n",
    "        data_pos.append(tweet[1])\n",
    "    else:\n",
    "        data_neg.append(tweet[1])\n",
    "\n",
    "train_pos = data_pos[:1100]\n",
    "train_neg = data_neg[:1100]\n",
    "\n",
    "token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                             token_pattern=token_pattern,\n",
    "                             binary=True)\n",
    "\n",
    "X2_train = vectorizer.fit_transform(train_pos+train_neg)\n",
    "y2_train = np.array([1]*len(train_pos)+[0]*len(train_neg))\n",
    "\n",
    "print(\"Vocabulary Size: %s\" % len(vectorizer.vocabulary_))\n",
    "print(\"Vectorizing Testing Text\")\n",
    "\n",
    "test_pos = data_pos[1100:]\n",
    "test_neg = data_neg[1100:]\n",
    "\n",
    "X2_test = vectorizer.transform(test_pos+test_neg)\n",
    "y2_test = np.array([1]*len(test_pos)+[0]*len(test_neg))\n",
    "\n",
    "print(\"Fitting Model\")\n",
    "\n",
    "mnbsvm = NBSVM()\n",
    "mnbsvm.fit(X2_train, y2_train)\n",
    "print('Test Accuracy: %s' % mnbsvm.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data = pd.read_csv('Earthquake_sensing_tweets.csv',header=0,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_url(tweet):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    if len(urls) !=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def has_magnitude(tweet):\n",
    "    decimal = re.findall(\"\\d+\\.\\d\\s\", tweet)\n",
    "    if len(decimal) != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data['has_magn']= tweet_sensing_data.Tweet_Text.apply(has_magnitude)\n",
    "tweet_sensing_data['has_url']= tweet_sensing_data.Tweet_Text.apply(find_url)\n",
    "\n",
    "tweet_sensing_data = remove_stopword(tweet_sensing_data)\n",
    "tweet_sensing_data = stemming (tweet_sensing_data)\n",
    "tweet_sensing_data = Feature_extraction_A(tweet_sensing_data)\n",
    "\n",
    "tweet_sensing_data = tweet_sensing_data.drop('nostopword',axis=1)\n",
    "tweet_sensing_data = tweet_sensing_data.drop('stem',axis=1)\n",
    "# tweet_sensing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataArray = tweet_sensing_data.values\n",
    "X3 = dataArray[:,3:]\n",
    "X3 = np.array(X3, dtype='float')\n",
    "y3 = dataArray[:,2]\n",
    "y3 = np.array(y3, dtype='float')\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(X3)\n",
    "X3 = scaler2.transform(X3)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range, kernel=k_options)\n",
    "grid2 = GridSearchCV(SVC(), param_grid=params_grids, cv=10)\n",
    "grid2.fit(X3_train,y3_train)\n",
    "\n",
    "print grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y3_pred = SVC(kernel=grid2.best_params_['kernel'] ,C=grid2.best_params_['C'], \n",
    "#              gamma=grid2.best_params_['gamma']).fit(X3_train,y3_train).predict(X3_test)\n",
    "\n",
    "clf2 =  SVC(kernel=grid2.best_params_['kernel'] ,C=grid2.best_params_['C'], \n",
    "             gamma=grid2.best_params_['gamma'])\n",
    "clf2.fit(X3_train,y3_train)\n",
    "y3_pred = clf2.predict(X3_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y3_test, y3_pred)\n",
    "sklearn.metrics.precision_score(y3_test, y3_pred)\n",
    "sklearn.metrics.recall_score(y3_test, y3_pred)\n",
    "sklearn.metrics.f1_score(y3_test, y3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Querying twitter real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For querying twitter real time, we are using the python 'tweepy' module. We are extracting 50 live streaming tweets from twitter and saving them in a 'live_data.csv' file for later classificaiton. To this 'live_data.csv' file, we are initially appending a set of 10 tweets, out of which 5 of them indicate a current earthquake and the other 5 of them, though related to earthquake, do not indicate a current earthquake. Our classifier needs to be able to detect the tweets which indicate only a current earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = '859531338973622277-fdJp7rien3doiULaof2DwcLwIzngo6k'\n",
    "access_token_secret = 'h3hAd5kzn9qRngThgQyRm9t2p1ErZH1orpAQ4HA15dlG9'\n",
    "consumer_key = 'wBJ2csLxjMCz0hRwWF7Pw826z'\n",
    "consumer_secret ='Au0OUtezSVY5VjKrBo9XTz9HJRHIQw2dJPtAVA4K1qBZgGGfh2'\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(StdOutListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.f = csv.writer(open(\"live_data.csv\", \"wb+\"))\n",
    "        self.f.writerow([\"Tweet_Text\"])\n",
    "        self.f.writerow([\"Are we having an earthquake?\"])\n",
    "        self.f.writerow([\"EARTHQUAKE?\"])\n",
    "        self.f.writerow([\"It shook like crazy #earthquake\"])\n",
    "        self.f.writerow([\"WOAHHHHH that was my first earthquake!!!!!\"])\n",
    "        self.f.writerow([\"Is it just me or was that an earthquake?\"])\n",
    "        self.f.writerow([\"An earthquake of mag 8.2 shook Delhi yesterday!!\"])\n",
    "        self.f.writerow([\"RT biggest earthquake in last ten years!! Mag 9.1 richter reported\"])\n",
    "        self.f.writerow([\"Attending an earthquake conference today.\"])\n",
    "        self.f.writerow([\"Japan has frequent earthquakes.\"])\n",
    "        self.f.writerow([\"Which is worse? Earthquake of 7.2 or 8.1?\"])\n",
    "        \n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 50:\n",
    "#             print self.num_tweets\n",
    "            tweet_data = json.loads(data)\n",
    "            if 'text' in tweet_data and tweet_data['lang'] == 'en':\n",
    "                self.f.writerow([tweet_data['text'].encode('utf-8')])\n",
    "                self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            print('Done extracting')\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(locations=[-180,-90,180,90])\n",
    "    \n",
    "get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "live_data = pd.read_csv('live_data.csv',header=0)\n",
    "\n",
    "for tweet in live_data.values.tolist():\n",
    "    temp_tweet = tweet[0]\n",
    "    temp_tweet_vector = vectorizer.transform([temp_tweet])\n",
    "    if mnbsvm.predict(temp_tweet_vector)[0] == 1:\n",
    "#         print temp_tweet\n",
    "        tweet_df = pd.DataFrame([temp_tweet],columns=['Tweet_Text'])\n",
    "        tweet_df['has_magn']= tweet_df.Tweet_Text.apply(has_magnitude)\n",
    "        tweet_df['has_url']= tweet_df.Tweet_Text.apply(find_url)\n",
    "\n",
    "        tweet_df = remove_stopword(tweet_df)\n",
    "        tweet_df = stemming (tweet_df)\n",
    "        tweet_df = Feature_extraction_A(tweet_df)\n",
    "\n",
    "        tweet_df = tweet_df.drop('nostopword',axis=1)\n",
    "        tweet_df = tweet_df.drop('stem',axis=1)\n",
    "#         print tweet_df\n",
    "        tempArray = tweet_df.values\n",
    "        temp_X = tempArray[:,1:]\n",
    "        temp_X = np.array(temp_X, dtype='float')\n",
    "\n",
    "        temp_X = scaler2.transform(temp_X)\n",
    "        temp_y = clf2.predict(temp_X)\n",
    "        if temp_y == 1:\n",
    "            print temp_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}