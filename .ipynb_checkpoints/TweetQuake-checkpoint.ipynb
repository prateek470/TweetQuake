{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Project \n",
    "TweetQuake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip())\n",
    "\n",
    "# data.head()\n",
    "X = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return nostop\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['no-stopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word))\n",
    "    return stemmed_word\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.Tweet_Text.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "    \n",
    "# Feature Extraction\n",
    "# X['count'] = X.Tweet_Text.str.split(' ').apply(len)#.value_counts()\n",
    "\n",
    "def has_earthquake(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1 or val[i].lower().find('quake') != -1 or val[i].lower().find('shaking') != -1:\n",
    "            return 1\n",
    "    return 0\n",
    "X['has_earthquake'] = X.Tweet_Text.str.split().apply(has_earthquake)\n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.Tweet_Text.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "#     X['feature_a'] = X.temp_1.astype(str) + ' words, the ' + X.temp_2.astype(str) + ' word'\n",
    "#     X = X.drop('temp_1', axis=1)\n",
    "#     X = X.drop('temp_2', axis=1)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_B(X):\n",
    "    X['feature_b'] = X.Tweet_Text.str.split().apply(remove_punc)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_C(X):\n",
    "    X['feature_c'] = X.Tweet_Text.str.split().apply(find_before_after_query_word)\n",
    "    return X\n",
    "\n",
    "def remove_punc(value):\n",
    "    punctuation_marks = re.compile(r'[.?!,\":;#-]')\n",
    "    words = []\n",
    "    for each in value:\n",
    "        words.append(punctuation_marks.sub('',each))\n",
    "    return ','.join(words)\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_before_after_query_word(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find('earthquake') != -1:\n",
    "            if i == 0 and len(val)>1:\n",
    "                return ','+val[i+1]\n",
    "            elif i == len(val)-1 and len(val)>1:\n",
    "                return val[i-1]+','\n",
    "            else:\n",
    "                return val[i-1]+','+val[i+1]\n",
    "    return ', '\n",
    "    \n",
    "# X['position'] = X.Tweet_Text.str.split().apply(find_position)\n",
    "# X['before_query1'] = 0\n",
    "# X.before_query1[X.position > 0] = X.position - 1\n",
    "X = remove_stopword(X)\n",
    "X = stemming (X)\n",
    "X = Feature_extraction_A(X)\n",
    "# X = Feature_extraction_B(X)\n",
    "# X = Feature_extraction_C(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import twitter, json\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "# results = api.GetSearch(\n",
    "#     raw_query=\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "# for each in results:\n",
    "# #     print(json.dumps(each, indent=2))\n",
    "#     print each\n",
    "\n",
    "# Find tweets using tweet ID\n",
    "res = api.GetStatus(389949367009808384)\n",
    "\n",
    "# Code to get the new data with Location and Created date\n",
    "# def find_created_at(id):\n",
    "#     try:\n",
    "#         res = api.GetStatus(id)\n",
    "#         return res.created_at\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['created_at'] = X.Tweet_ID.astype(int).apply(find_created_at)\n",
    "# def find_location(id):\n",
    "#     try:\n",
    "#         return api.GetStatus(id).user.location\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['location'] = X.Tweet_ID.astype(int).apply(find_location)\n",
    "# X.to_csv('new_data.csv', sep=',',encoding='utf-8')\n",
    "\n",
    "y = data['Info'].values\n",
    "X = X.drop('Tweet_ID',axis=1)\n",
    "X = X.drop('Tweet_Text',axis=1)\n",
    "\n",
    "X = X.drop('no-stopword',axis=1)\n",
    "X = X.drop('stem',axis=1)\n",
    "\n",
    "\n",
    "X = X.drop('Info',axis=1)\n",
    "X\n",
    "# X = X.drop('total_words',axis=1)\n",
    "# X = X.drop('position_query_word',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration of Twitter API\n",
    "Keys are for reference \n",
    "We can get new data set using this\n",
    "or modify the old data set to get some new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter, json\n",
    "api = twitter.Api(consumer_key='4j8Uk7Hea3pdgEuJ6nkvqvVYO',\n",
    "                  consumer_secret='VU4nTJFz4KQSr66Q1MH7snV0BFEkkeL8sOnu5qGSfzU9poUSU1',\n",
    "                  access_token_key='44338623-psdoVV5cnUnS9TN0fgrtt4KoMfwxXGfevzS5CllRu',\n",
    "                  access_token_secret='VKioM8alAKMPTlE1BauuzLC1SLtXbpDWZZ6qEDPi8xz3F')\n",
    "results = api.GetSearch(\n",
    "    raw_query=\"q=-earthquake%2C%20-shaking%2C%20-quake%2C%20-tremor%20since%3A2010-09-15%20until%3A2017-04-23&src=typd&lang=en\")#\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "\n",
    "# f = csv.writer(open(\"test_neg1.csv\", \"wb+\"))\n",
    "# f.writerow([\"tweet_id\", \"tweet_text\", \"created_at\", \"url\", \"location\",\"media\"])\n",
    "current_id = ''\n",
    "# print results\n",
    "for x in results:\n",
    "    print x.text\n",
    "    x.text = x.text.encode('utf-8').strip()\n",
    "    x.user.location = x.user.location.encode('utf-8').strip()\n",
    "#     if x.media is not None:\n",
    "#     x.media = x.media.encode('utf-8').strip()\n",
    "#     if x.urls is not None and x.media is not None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,1,x.user.location,1])\n",
    "#     elif x.urls is None and x.media is not None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,0,x.user.location,1])\n",
    "#     elif x.urls is not None and x.media is None:\n",
    "#         f.writerow([x.id,x.text,x.created_at,1,x.user.location,0])\n",
    "#     else:\n",
    "#         f.writerow([x.id,x.text,x.created_at,0,x.user.location,0])\n",
    "#     f.writerow([x.id,x.text,x.created_at,x.urls,x.user.location,x.media])\n",
    "    current_id = x.id\n",
    "count = 0\n",
    "for i in range(0):\n",
    "    results = api.GetSearch(\n",
    "        raw_query=\"q=-earthquake%2C%20-shaking%2C%20-quake%2C%20-tremor%20since%3A2010-11-18%20until%3A2017-04-23&src=typd&count=100&lang=en&since_id\"+str(current_id))#\"q=earthquake%20&result_type=recent&since=2016-07-19&count=10&lang=en\")\n",
    "    for x in results:\n",
    "        x.text = x.text.encode('utf-8').strip()\n",
    "        x.user.location = x.user.location.encode('utf-8').strip()\n",
    "#         x.media = x.media.encode('utf-8').strip()\n",
    "#         if x.urls is not None and x.media is not None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,1,x.user.location,1])\n",
    "#         elif x.urls is None and x.media is not None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,0,x.user.location,1])\n",
    "#         elif x.urls is not None and x.media is None:\n",
    "#             f.writerow([x.id,x.text,x.created_at,1,x.user.location,0])\n",
    "#         else:\n",
    "#             f.writerow([x.id,x.text,x.created_at,0,x.user.location,0])\n",
    "        f.writerow([x.id,x.text,x.created_at,x.urls,x.user.location,x.media])\n",
    "        current_id = x.id\n",
    "        count += 1\n",
    "#     print count\n",
    "\n",
    "# Find tweets using tweet ID\n",
    "res = api.GetStatus(389949367009808384)\n",
    "\n",
    "# Code to get the new data with Location and Created date\n",
    "# def find_created_at(id):\n",
    "#     try:\n",
    "#         res = api.GetStatus(id)\n",
    "#         return res.created_at\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['created_at'] = X.Tweet_ID.astype(int).apply(find_created_at)\n",
    "# def find_location(id):\n",
    "#     try:\n",
    "#         return api.GetStatus(id).user.location\n",
    "#     except:\n",
    "#         return ''\n",
    "# X['location'] = X.Tweet_ID.astype(int).apply(find_location)\n",
    "# X.to_csv('new_data.csv', sep=',',encoding='utf-8')\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "# Default start_time to one minute ago\n",
    "start_time = datetime.utcnow() - timedelta(minutes=1)\n",
    "\n",
    "try:\n",
    "    # Loop forever\n",
    "    while False:\n",
    "        # Construct the USGS API URL by including start_time as a parameter\n",
    "        usgs_url = \"http://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={0}\".format(start_time.isoformat())\n",
    "        print(usgs_url)\n",
    "        # Get the response and extract the \"features\" element\n",
    "        response = requests.get(usgs_url)\n",
    "        earthquakes = response.json()['features']\n",
    "\n",
    "        # Reset start_time to the current time so we how far back to pull data on the next iteration of the loop\n",
    "        start_time = datetime.utcnow()\n",
    "        print(start_time)\n",
    "\n",
    "        # Loop through each of the earthquakes that were returned\n",
    "        for earthquake in [e['properties'] for e in earthquakes]:\n",
    "            # Construct the tweet\n",
    "            tweet_text = \"Magnitude {0} earthquake {1}\\n{2}\".format(earthquake['mag'], earthquake['place'], earthquake['url'])\n",
    "            print(tweet_text)\n",
    "            # Post it\n",
    "#             twitter.update_status(tweet_text)\n",
    "            # Sleep for a minute so we don't spam Twitter in case of many earthquakes at once\n",
    "            time.sleep(60)\n",
    "\n",
    "        # Sleep for an hour so we don't spam Twitter\n",
    "        time.sleep(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error ocurred\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SVM model implementation\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# vec = DictVectorizer()\n",
    "# df = vec.fit_transform(X.feature_a.str).toarray()\n",
    "# X = pd.get_dummies(X.feature_b.str.split())\n",
    "\n",
    "def standardizedX(X):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    standardizedX = scaler.transform(X)\n",
    "    return standardizedX\n",
    "# Tune hyperparameter gamma and choose best gamma for model training\n",
    "def hyperparameter_tuning(X, y):\n",
    "\t# Choose value of hyper parameter from below values of gamma\n",
    "    gammas = [2**-1, 2**-3, 2**-5, 2**-7, 2**-9]\n",
    "    classifier = GridSearchCV(estimator=svm.SVR(), cv=10, param_grid=dict(gamma=gammas))\n",
    "\n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier.fit(X_train, y_train)\n",
    "    return classifier\n",
    "\n",
    "# 10- fold cross validation and error evaluation\n",
    "# Loop of 30 to see 300 validations with shuffle on and off\n",
    "# Loop of 10 to check the confidence interval - paper doesn't describe much on the \n",
    "# confidence interval. This works only for shuffle On.\n",
    "def cross_validation_evaluation(X, y):\n",
    "    mean_error, mad_error = 0, 0\n",
    "    count = 0\n",
    "    mean_min, mad_min = 100, 100\n",
    "    mean_max, mad_max = 0, 0\n",
    "    f1_sco,accuracy_sco = 0,0\n",
    "#     classifier = hyperparameter_tuning(X, y)\n",
    "    model=svm.SVC(kernel='linear', gamma=0.001)#classifier.best_estimator_.gamma)\n",
    "    \n",
    "    for j in range(1):\n",
    "        for i in range(10):\n",
    "            kf = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "#             print len(X)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                count += 1\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "#                 mean_error += mean_squared_error(y_test, model.predict(X_test))\n",
    "#                 mad_error += mean_absolute_error(y_test,model.predict(X_test))\n",
    "#                 print (y_test,model.predict(X_test))\n",
    "#                 print (model.predict(X_test))\n",
    "                f1_sco += f1_score(y_test,model.predict(X_test),pos_label='related')\n",
    "                accuracy_sco += accuracy_score(y_test,model.predict(X_test),normalize=True)\n",
    "#                 print classification_report(y_test,model.predict(X_test))\n",
    "#         mean_min = min(mean_min, (mean_error/count)**0.5)\n",
    "#         mean_max = max(mean_max, (mean_error/count)**0.5)\n",
    "#         mad_min = min(mad_min, (mad_error/count))\n",
    "#         mad_max = max(mad_max, (mad_error/count))\n",
    "#     RMSE = (mean_error/count)**0.5\n",
    "#     MAD = mad_error/count\n",
    "    print 'F1-score: ' + str(f1_sco/count)\n",
    "    print 'Accuracy: ' + str(accuracy_sco/count)\n",
    "#     return RMSE, MAD, mean_min, mean_max, mad_min, mad_max\n",
    "    \n",
    "\n",
    "# X = standardizedX(X)\n",
    "# print X.tail()\n",
    "cross_validation_evaluation(X.values, y)\n",
    "# RMSE, MAD, mean_min, mean_max, mad_min, mad_max = cross_validation_evaluation(X.values, y)\n",
    "# print \"For feature: \"\n",
    "# print 'RMSE +- Confidence Interval: '+\"{0:.2f}\".format(RMSE)+' +- '+\"{0:.2f}\".format(max(RMSE-mean_min, mean_max-RMSE))\n",
    "# print 'MAD +- Confidence Interval: '+\"{0:.2f}\".format(MAD)+' +- '+\"{0:.2f}\".format(max(MAD-mad_min, mad_max-MAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}