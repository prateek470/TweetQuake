{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Final Project\n",
    "## TweetQuake\n",
    "Detection of Earthquake using Twitter data.\n",
    "Twitter users are everywhere, check the twitter map and earthquake frequency map\n",
    "<img src=\"img/TwitterMap.png\" alt=\"Drawing\" style=\"width: 475px;float:left; margin-top: 30px\" title = \"Twitter Map\"/>\n",
    "<img src=\"img/EarthQuakeMap.png\" alt=\"Drawing\" style=\"width: 475px;float:right\" title = \"Earthquake Map\"/>\n",
    "<img src=\"img/EarthQuakeTweet.png\" alt=\"Drawing\" style=\"width: 600px;\" title = \"Earthquake Tweet Frequency\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Problem Statement\n",
    "\n",
    "Online Social Media have become an important alternate channel to spread information during natural disasters and other important events. Twitter is one such channel where people look forward to knowing about their friends and relatives after a catastrophic event such as an Earthquake, tornado, and others. The real-time nature of Twitter enables it to be an important source for information extraction. In a case of an earthquake, people who experience it, tweet about it to inform their friends about their safety and to spread the news of a target event. In this report, we have investigated the extraction of event information about Earthquake using the service API of Twitter. Our algorithm classifies Tweets on the basis of whether they are relevant to Earthquake and if yes, whether they are indicative of a current earthquake.\n",
    "\n",
    "\n",
    "Earlier, the main source of information for an earthquake like event was the radio, however, there was some time lapse from the occurrence of an event and the announcement and also the reach of information was not global. In the past few years, people especially youngsters are turning to Twitter to learn about such events. Twitter has the advantage of fast propagation speed and more number of contributors when compared to radio. \n",
    "\n",
    "Twitter is categorized as a microblogging service, ie a form of blogging that enables users to send brief text and multimedia updates. Other similar services are Tumblr, Plurk etc. We focus on examining the data received from Twitter due to its popularity and data volume. There are 190 million people who use Twitter per month, generating 65 million tweets per day[1] . Compared to other microblogging sites, Twitter user updates their account more often and tweet regularly, often several times in a day. For example, when an airplane crash-landed on the Hudson River in New York, the first published report was via Twitter. Similarly, many events such as presidential campaigns, football games, and others are being analyzed more often via social media tools such as Twitter rather than radio or News channel. \n",
    "\n",
    "Generally, people who experience an event such as Earthquake are the first to report it via Twitter, and the tweets spread out even before the event is registered with the USGS and long before it is covered in news channels. The motivation behind our research was:\n",
    "\tCan we detect such event occurrence in real-time by monitoring tweets?\n",
    "The amount of real-time information flow via Twitter motivated us to design a real-time automated Earthquake Detection Classifier. \n",
    "\n",
    "## Related Work.\n",
    "\n",
    "Event or disaster detection using twitter is one of the fastest and effective way to get the relevant information. Many researchers used the tweets to get the information about the trends, social media relations of twitter users, retweet activities. Earthquake detection using tweets is first observed in 2010 (Tweet Analysis for Real-Time Event Detection and Earthquake Reporting System Development\n",
    ")[8]\n",
    "Since than USGS developed an application to effectively detect an earthquake using as minimum as 14 tweets. Currently, twitter is being used to detect an earthquake along with other disasters. \\\n",
    "\n",
    "Previously many researchers have published studies of twitter and observed the network structure of Twitter [2], [3], [4]. Some of the researchers used social media as of the characteristics to analyze the Twitter data [5], [6]. In papers [7] and [8], authors have development different applications using the tweet and other relevant data of Twitter and developed some real time applications.\n",
    "\n",
    "\n",
    "Most of the papers on event detection use basic features like presence of a query word like earthquake or shaking along with the length of tweet (as the person experiencing tweet will not write a big and fancy tweet). The accuracy of the previous event detection as described in the paper [8] is just 76% in best case. We have improved the feature selection process and used different classifiers to detect the earthquake relevant tweets and current earthquake related tweets to improve the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "def length(df):\n",
    "    return len(df['Tweet_Text'])\n",
    "\n",
    "\n",
    "data = get_data('2013_Bohol_earthquake-tweets_labeled.csv')\n",
    "data['Info'] = 'related'\n",
    "data.Info[('Not related' == data.Informativeness)] = 'not-related'\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(lambda x: x.decode('unicode_escape').\\\n",
    "                                          encode('ascii', 'ignore').\\\n",
    "                                          strip().lower())\n",
    "\n",
    "X1 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "y1 = data.Info\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wordmap = defaultdict(int)\n",
    "\n",
    "def Stopword(tweet):\n",
    "    nostop = []\n",
    "    for word in tweet:\n",
    "        #word = word.decode('utf-8')\n",
    "        if word not in stopwords: nostop.append(word)\n",
    "    return ' '.join(nostop)\n",
    "\n",
    "def remove_stopword (X):\n",
    "    X['nostopword'] = X.Tweet_Text.str.split().apply(Stopword)\n",
    "    return X\n",
    "\n",
    "def Porter_Stem(tweet):\n",
    "    stemmed_word = []\n",
    "    for word in tweet:\n",
    "        word = word.decode('utf-8')\n",
    "        stemmed_word.append(porter_stemmer.stem(word)) \n",
    "    return ' '.join(stemmed_word)\n",
    "            \n",
    "def stemming (X):\n",
    "    X['stem'] = X.nostopword.str.split().apply(Porter_Stem)\n",
    "    return X\n",
    "\n",
    "def update_wordmap(tweet):\n",
    "    #update wordmap\n",
    "    for word in tweet:\n",
    "        wordmap[word]+=1\n",
    "\n",
    "def term_frequency_plot(X):\n",
    "    #Plot a graph for term frequency in tweets\n",
    "    X.stem.str.split().apply(update_wordmap)\n",
    "    sorted_x = sorted(wordmap.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    objects = list()\n",
    "    freq = list()\n",
    "    for i in range(10):\n",
    "        objects.append(sorted_x[i][0])\n",
    "        freq.append(sorted_x[i][1])\n",
    "\n",
    "    x_pos = np.arange(len(objects))\n",
    "    plt.barh(x_pos, freq, align='center', alpha=0.5)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.yticks(x_pos, objects)\n",
    "    plt.title('Term Frequency usage')\n",
    "    plt.show()\n",
    "    \n",
    "def Feature_extraction_A(X):\n",
    "    X['total_words'] = X.stem.str.split(' ').apply(len)\n",
    "    X['position_query_word'] = X.stem.str.split().apply(find_position)\n",
    "    return X\n",
    "\n",
    "def Feature_extraction_BnC(X):\n",
    "    word_features = X[['Tweet_ID','Tweet_Text','Info']]\n",
    "    word_features = word_features.values.tolist()\n",
    "\n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "\n",
    "    for tweet in word_features:\n",
    "        if tweet[2] == 'related':\n",
    "            data_pos.append(tweet[1])\n",
    "        else:\n",
    "            data_neg.append(tweet[1])\n",
    "\n",
    "    token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 token_pattern=token_pattern,\n",
    "                                 binary=True,\n",
    "                                max_features=5000)\n",
    "    word_vector = vectorizer.fit_transform(data_pos+data_neg)\n",
    "    \n",
    "    return X.join(pd.DataFrame(word_vector.toarray()))\n",
    "\n",
    "def find_position(val):\n",
    "    for i in range(len(val)):\n",
    "        if val[i].lower().find(porter_stemmer.stem('earthquake')) != -1:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "X1 = remove_stopword(X1)\n",
    "X1 = stemming (X1)\n",
    "term_frequency_plot(X1)\n",
    "X1 = Feature_extraction_A(X1)\n",
    "# X1 = Feature_extraction_BnC(X1)\n",
    "\n",
    "y1 = data['Info'].values\n",
    "X1 = X1.drop('Tweet_ID',axis=1)\n",
    "X1 = X1.drop('Tweet_Text',axis=1)\n",
    "X1 = X1.drop('nostopword',axis=1)\n",
    "X1 = X1.drop('stem',axis=1)\n",
    "X1 = X1.drop('Info',axis=1)\n",
    "X1 = X1.values\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X1)\n",
    "type(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(X1)\n",
    "X1 = scaler1.transform(X1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "# k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range)\n",
    "grid1 = GridSearchCV(SVC('rbf'), param_grid=params_grids, cv=10)\n",
    "grid1.fit(X1_train,y1_train)\n",
    "\n",
    "print grid1.best_params_\n",
    "# grid1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = SVC(kernel='rbf' ,C=grid1.best_params_['C'], \n",
    "             gamma=grid1.best_params_['gamma'])\n",
    "\n",
    "clf1.fit(X1_train,y1_train)\n",
    "y1_pred = clf1.predict(X1_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y1_test, y1_pred)\n",
    "sklearn.metrics.precision_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.recall_score(y1_test, y1_pred,pos_label='related')\n",
    "sklearn.metrics.f1_score(y1_test, y1_pred,pos_label='related')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive-Bayes SVM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes SVM (NBSVM) is a simple but novel SVM variant using NB log-count ratios as feature values and is supposed to be a robust performer. This model is an interpolation between MNB and SVM, which can be seen as a form of regularization: trust NB unless the SVM is very confident. \n",
    "\n",
    "The concept of NBSVM has been obtained from this research paper : Wang, Sida, and Christopher D. Manning. \"Baselines and bigrams: Simple, good sentiment and topic classification.\" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 2012.\n",
    "\n",
    "The class implementation of the NBSVM has been obtained from this repository: https://github.com/Joshua-Chin/nbsvm.git\n",
    "\n",
    "For the first classification, where we are classifying earthquake-relevant tweets from earthquake-irrelevant tweets, we are using the NBSVM classifier as it's giving us a higher accuracy compared to the SVM classifier discussed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import spmatrix, coo_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "\n",
    "    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.C = C\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        if len(self.classes_) == 2:\n",
    "            coef_, intercept_ = self._fit_binary(X, y)\n",
    "            self.coef_ = coef_\n",
    "            self.intercept_ = intercept_\n",
    "        else:\n",
    "            coef_, intercept_ = zip(*[\n",
    "                self._fit_binary(X, y == class_)\n",
    "                for class_ in self.classes_\n",
    "            ])\n",
    "            self.coef_ = np.concatenate(coef_)\n",
    "            self.intercept_ = np.array(intercept_).flatten()\n",
    "        return self\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n",
    "        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n",
    "        p = np.asarray(p,dtype=np.float)\n",
    "        q = np.asarray(q,dtype=np.float)\n",
    "        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n",
    "        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n",
    "\n",
    "        if isinstance(X, spmatrix):\n",
    "            indices = np.arange(len(r))\n",
    "            r_sparse = coo_matrix(\n",
    "                (r, (indices, indices)),\n",
    "                shape=(len(r), len(r))\n",
    "            )\n",
    "            X_scaled = X * r_sparse\n",
    "        else:\n",
    "            X_scaled = X * r\n",
    "\n",
    "        lsvc = LinearSVC(\n",
    "            C=self.C,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            max_iter=10000\n",
    "        ).fit(X_scaled, y)\n",
    "\n",
    "        mean_mag =  np.abs(lsvc.coef_).mean()\n",
    "\n",
    "        coef_ = (1 - self.beta) * mean_mag * r + \\\n",
    "                self.beta * (r * lsvc.coef_)\n",
    "\n",
    "        intercept_ = (1 - self.beta) * mean_mag * b + \\\n",
    "                     self.beta * lsvc.intercept_\n",
    "\n",
    "        return coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are diving the data into a train-test split of 80-20. We are building our vocabulary based on the training data using scikit-learn's method TfidfVectorizer. We are using unigram, bigram and trigram features and the Tf-Idf weighting scheme on the word vector. We are setting the tf term in tf-idf to be binary, as it was increasing our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Vectorizing Training Text\")\n",
    "\n",
    "X2 = data[['Tweet_ID','Tweet_Text','Info']]\n",
    "X2 = X2.values.tolist()\n",
    "\n",
    "data_pos = []\n",
    "data_neg = []\n",
    "\n",
    "for tweet in X2:\n",
    "    if tweet[2] == 'related':\n",
    "        data_pos.append(tweet[1])\n",
    "    else:\n",
    "        data_neg.append(tweet[1])\n",
    "\n",
    "train_pos = data_pos[:1100]\n",
    "train_neg = data_neg[:1100]\n",
    "\n",
    "token_pattern = r'\\w+|[%s]' % string.punctuation\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                             token_pattern=token_pattern,\n",
    "                             binary=True)\n",
    "\n",
    "X2_train = vectorizer.fit_transform(train_pos+train_neg)\n",
    "y2_train = np.array([1]*len(train_pos)+[0]*len(train_neg))\n",
    "\n",
    "print(\"Vocabulary Size: %s\" % len(vectorizer.vocabulary_))\n",
    "print(\"Vectorizing Testing Text\")\n",
    "\n",
    "test_pos = data_pos[1100:]\n",
    "test_neg = data_neg[1100:]\n",
    "\n",
    "X2_test = vectorizer.transform(test_pos+test_neg)\n",
    "y2_test = np.array([1]*len(test_pos)+[0]*len(test_neg))\n",
    "\n",
    "print(\"Fitting Model\")\n",
    "\n",
    "mnbsvm = NBSVM()\n",
    "mnbsvm.fit(X2_train, y2_train)\n",
    "print('Test Accuracy: %s' % mnbsvm.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Part 2: Classifying \"Earthquake Sensing\"  vs. \"Earthquake Relevant\" Tweets\n",
    "\n",
    "In the analysis up until now, we have been able to find and classify tweets which are *earthquake-relevant* vs *earthquake-irrelevant* tweets. However that is not enough to be able to detect earthquakes real-time. Many tweets that are *earthquake-relevant* are tweets in the aftermath of an earthquake when the occurence has been detected much before by seismic sensors and has been distributed by media houses and the internet. Many *earthquake-relevant* tweets might just be information about earthquakes or conferences and what not. Hence the next crucial step is to classify these 2 classes. \n",
    "\n",
    "*Positive Class - Earthquake Sensing Tweets*\n",
    "\n",
    "*Negative Class- (only)Earthquake relevant Tweets*\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "For this part of the analysis we did not find any data readily available for the *positive class*. For the *negative class* we could just use the data from the earlier dataset. So after trying and failing to extract data from twitter api and trying to sift through that and tag it, we *manually* scraped data off twitter and recorded it in a csv. **This task required significant effort**. We collected tweets by searching for known earthquake timings and locations and searched twitter for tweets which happened within 2-3 minutes of the earthquake. This would serve as the *positive class* for this training set. The total size of this data is ~ 800 tweets\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "If one eyeballs tweets from the *positive class* one sees that almost all of them contain the word earthquake. Also most of these tweets are small because when people react in the moment, they typically are too surprised to churn out long tweets. Thus *Feature 1* from previous analysis , i.e length of tweet and position of word \"earthquake\" are good features. This is what the paper we referred to had also used.\n",
    "\n",
    "In addition to those, **we came up with a few features of our own. This is another contribution above and beyond what was reported in the research paper**. \n",
    "\n",
    "1. If one peruses through the tweets, one sees that the tweets tweeted in the moments the earthquake is happening will almost never have any *url link*. There are 2 reasons. One is that again the user is too startled to cite stuff while experiencing an earthquake. Secondly since the first tweeters are by name the first people to know or experience the earthquake, there is no formal news or information available at that moment to refer to via links. In contrast many tweets which are relevant but not sensing are tweets done in the afterhours of an earthquake. Here many people link to either news articles, videos, other information on the internet. Hence *the presence/absence of url/links* is vital discriminatory information between the 2 classes, thus we make this a feature. We used regex to find http in our tweets.\n",
    "\n",
    "\n",
    "2. Similarly when users are tweeting about an earthquake as it is happening, nobody has any quantitative information regarding the magnitude/strength of the earthquake. Thus tweets would hardly have any numerical information except probably the time of occurence or other qualifiers like which time the earthquake is happening in a few months or years. BUT when tweets are tweeted in the afterhours of the earthquake, most of them contain the magnitude of the earthquake which is common information at that point of time. Hence again *the presence/absence of magnitude* is a pretty strong discriminator between positive and negative clases. Thus we chose this a a feature too. We use regex to find tweets with decimal values in tweets, which we believe (and saw from data) that it was a pretty good surrogate to finding magnitude specifically.\n",
    "\n",
    "**Classification and Evaluation**\n",
    "\n",
    "For classifiers we tried Support Vector Machines and Random Forests on our 4-feature dataset. Both these algorithms have many hyperparameters to tune and for that we used K-fold cross validation accuracy score. This was performed on only the *training data* (80% of total data) to find the best hyperparameter set. Then the complete training data was trained over this set of hyperparameters. Finally to evaluate performance, we tested the final classifer on our *test data* (20% of total data). Our performance from both classifiers was similar, and the SVM just performed marginally better. So we only included that in our analysis below.\n",
    "\n",
    "For evaluation we primarly check accuracy but also report precision, recall and F1-score (balanced f-score). For this classifier recall is more important that precision. This is because the recall controls how early (minimum number of tweets) we are able to raise an alarm given users start tweeting about an earthquake. Thus we want to be able to find maximum out of all such tweets. A bad precision will just lead to false alarms, the cost of which is significantly less than missing out on many tweets in an attempt to be precise. However our classifier performs equally good on all counts with more than 85% accuracy and almost similar (~85%) precision and recall. Thus it will pick 4 out of 5 tweets about an earthquake correctly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data = pd.read_csv('Earthquake_sensing_tweets.csv',header=0,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_url(tweet):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    if len(urls) !=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def has_magnitude(tweet):\n",
    "    decimal = re.findall(\"\\d+\\.\\d\\s\", tweet)\n",
    "    if len(decimal) != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sensing_data['has_magn']= tweet_sensing_data.Tweet_Text.apply(has_magnitude)\n",
    "tweet_sensing_data['has_url']= tweet_sensing_data.Tweet_Text.apply(find_url)\n",
    "\n",
    "tweet_sensing_data = remove_stopword(tweet_sensing_data)\n",
    "tweet_sensing_data = stemming (tweet_sensing_data)\n",
    "tweet_sensing_data = Feature_extraction_A(tweet_sensing_data)\n",
    "\n",
    "tweet_sensing_data = tweet_sensing_data.drop('nostopword',axis=1)\n",
    "tweet_sensing_data = tweet_sensing_data.drop('stem',axis=1)\n",
    "# tweet_sensing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataArray = tweet_sensing_data.values\n",
    "X3 = dataArray[:,3:]\n",
    "X3 = np.array(X3, dtype='float')\n",
    "y3 = dataArray[:,2]\n",
    "y3 = np.array(y3, dtype='float')\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(X3)\n",
    "X3 = scaler2.transform(X3)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 5, base=2)\n",
    "gamma_range = np.logspace(-9, 1, 5, base=2)\n",
    "k_options = ['linear','poly','rbf']\n",
    "params_grids = dict(gamma=gamma_range, C=C_range, kernel=k_options)\n",
    "grid2 = GridSearchCV(SVC(), param_grid=params_grids, cv=10)\n",
    "grid2.fit(X3_train,y3_train)\n",
    "\n",
    "print grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2 =  SVC(kernel=grid2.best_params_['kernel'] ,C=grid2.best_params_['C'], \n",
    "             gamma=grid2.best_params_['gamma'])\n",
    "clf2.fit(X3_train,y3_train)\n",
    "y3_pred = clf2.predict(X3_test)\n",
    "\n",
    "sklearn.metrics.accuracy_score(y3_test, y3_pred)\n",
    "sklearn.metrics.precision_score(y3_test, y3_pred)\n",
    "sklearn.metrics.recall_score(y3_test, y3_pred)\n",
    "sklearn.metrics.f1_score(y3_test, y3_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Putting it together: Querying twitter real-time and finding \"earthquake sensing tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For querying twitter real time, we are using the python 'tweepy' module. We are extracting 50 live streaming tweets from twitter and saving them in a 'live_data.csv' file for later classificaiton. To this 'live_data.csv' file, we are initially appending a set of 10 tweets, out of which 5 of them indicate a current earthquake and the other 5 of them, though related to earthquake, do not indicate a current earthquake. Our classifier needs to be able to detect the tweets which indicate only a current earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = '859531338973622277-fdJp7rien3doiULaof2DwcLwIzngo6k'\n",
    "access_token_secret = 'h3hAd5kzn9qRngThgQyRm9t2p1ErZH1orpAQ4HA15dlG9'\n",
    "consumer_key = 'wBJ2csLxjMCz0hRwWF7Pw826z'\n",
    "consumer_secret ='Au0OUtezSVY5VjKrBo9XTz9HJRHIQw2dJPtAVA4K1qBZgGGfh2'\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(StdOutListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.f = csv.writer(open(\"live_data.csv\", \"wb+\"))\n",
    "        self.f.writerow([\"Tweet_Text\"])\n",
    "        self.f.writerow([\"Are we having an earthquake?\"])\n",
    "        self.f.writerow([\"EARTHQUAKE?\"])\n",
    "        self.f.writerow([\"It shook like crazy #earthquake\"])\n",
    "        self.f.writerow([\"WOAHHHHH that was my first earthquake!!!!!\"])\n",
    "        self.f.writerow([\"Is it just me or was that an earthquake?\"])\n",
    "        self.f.writerow([\"An earthquake of mag 8.2 shook Delhi yesterday!!\"])\n",
    "        self.f.writerow([\"RT biggest earthquake in last ten years!! Mag 9.1 richter reported\"])\n",
    "        self.f.writerow([\"Attending an earthquake conference today.\"])\n",
    "        self.f.writerow([\"Japan has frequent earthquakes.\"])\n",
    "        self.f.writerow([\"Which is worse? Earthquake of 7.2 or 8.1?\"])\n",
    "        \n",
    "\n",
    "    def on_data(self, data):\n",
    "        if self.num_tweets < 50:\n",
    "#             print self.num_tweets\n",
    "            tweet_data = json.loads(data)\n",
    "            if 'text' in tweet_data and tweet_data['lang'] == 'en':\n",
    "                self.f.writerow([tweet_data['text'].encode('utf-8')])\n",
    "                self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            print('Done extracting')\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(locations=[-180,-90,180,90])\n",
    "    \n",
    "get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "live_data = pd.read_csv('live_data.csv',header=0)\n",
    "\n",
    "print('Tweets that indicate a current earthquake : \\n')\n",
    "\n",
    "for tweet in live_data.values.tolist():\n",
    "    temp_tweet = tweet[0]\n",
    "    temp_tweet_vector = vectorizer.transform([temp_tweet])\n",
    "    if mnbsvm.predict(temp_tweet_vector)[0] == 1:\n",
    "#         print temp_tweet\n",
    "        tweet_df = pd.DataFrame([temp_tweet],columns=['Tweet_Text'])\n",
    "        tweet_df['has_magn']= tweet_df.Tweet_Text.apply(has_magnitude)\n",
    "        tweet_df['has_url']= tweet_df.Tweet_Text.apply(find_url)\n",
    "\n",
    "        tweet_df = remove_stopword(tweet_df)\n",
    "        tweet_df = stemming (tweet_df)\n",
    "        tweet_df = Feature_extraction_A(tweet_df)\n",
    "\n",
    "        tweet_df = tweet_df.drop('nostopword',axis=1)\n",
    "        tweet_df = tweet_df.drop('stem',axis=1)\n",
    "#         print tweet_df\n",
    "        tempArray = tweet_df.values\n",
    "        temp_X = tempArray[:,1:]\n",
    "        temp_X = np.array(temp_X, dtype='float')\n",
    "\n",
    "        temp_X = scaler2.transform(temp_X)\n",
    "        temp_y = clf2.predict(temp_X)\n",
    "        if temp_y == 1:\n",
    "            print temp_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}